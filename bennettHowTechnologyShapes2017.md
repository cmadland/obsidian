---
DOI: 10.1111/bjet.12439
Date: 2017
Rating: 0/5
Title: "How technology shapes assessment design : Findings from a study of university teachers"
ShortSummary: ""
annotation-target: bennettHowTechnologyShapes2017.pdf
---


#### [How technology shapes assessment design : Findings from a study of university teachers](bennettHowTechnologyShapes2017.pdf)




> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Bennett, S., Dawson, P., Bearman, M., Molloy, E., & Boud, D. (2017). How technology shapes assessment design: Findings from a study of university teachers. _British Journal of Educational Technology_, _48_(2), 672–682. [https://doi.org/10.1111/bjet.12439](https://doi.org/10.1111/bjet.12439)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 


#### Related

#### Annotations


Technologies to support assessment have a long history in higher educationfrom the early days of programmed instruction and computer-based quizzes, to richer forms of interaction and content creation underpinned by constructivist approaches, and more recent tools that support online assignment submission, peer- and self-assessment, integrity checking and marking (Buckley & Cowap, 2013; Kulkarni et al, 2013; Schmid et al, 2009; Tamim, Bernard, Borokhovski, Abrami & Schmid, 2011). 

 

Designers of assessment who depart from established practice risk complaints from students and criticism from colleagues. This may encourage conservatism in assessment design, particularly if integrating new technology tools is perceived as increasing the risks (Carless, 2009). But as with educational technology more generally, the reasons for limited adoption are poorly understood and warrant further scholarly investigation. 

 

Prior research into technology in assessment has tended to focus on how learners interact wit 

 

particular technologies, often through detailed case studies of innovative projects. This work has been important in providing accounts of how emerging technologies might be integrated to support student learning, and identifying specific obstacles that might need to be addressed. There are two ways in which this body of work needs to be extended. The first is to specifically investigate the perspectives of university teachers who are responsible for assessment design. This would add specific consideration of issues related to assessment to current understanding of how and why teachers integrate technology into their teaching (see, eg, Jump, 2011; Kirkwood & Price, 2013). In addition, we need a broader account of technology integration in assessment that moves beyond specific projects by technology innovators and seeks to understand teachers’ experiences in the context of more routine assessment design work. The need for accounts of educator experiences aligns with Selwyn’s (2010) argument for research into the “state of the actual” in technology integration “concerning what is actually taking place when a digital technology meets an educational setting” (p. 70). 

 

Theme 1: The “economics” of assessment drove adoption of technology to support assessment Time and moneyconstraints featured prominentlyin participants’ references to technology.This was particularlythe case for large classes. Many felt there was pressure to adopt more apparently efficient forms of assessment, such as online multiple-choice quizzes that could provide automatic feedback tostudents 

 

The ease of setting up and administering online quizzes was also attractive. A move to increasing use of online quizzes was evident across all discipline groupings, with efficiency cited as the main driver. 

 

Technology-supported forms of assessment also conferred other administrative benefits; for example, online submissions were stored centrally and could easily be referred to and retrieved. 

 

The need for laborsaving technologies influenced what university teachers considered possible and preferable in assessment design, but could lead to unanticipated consequences due to inexperience or lack of foresight. 

 

Theme 2: Technology-supported assessment is considered contemporary and innovative 

 

There was a sense from many participants that technology is a contemporary approach to assessment that is inevitably gaining momentum in higher education. 

 

In some cases, interviewees expressed a clear rationale for pedagogical improvement through the introductionof technology.In othercases,theapproacheswereshapedmorebythetoolsavailable 

 

Participants expressed frustration at the lack of time to do “something more interesting.” Technology integration became a secondary consideration in their design processes 

 

For one participant, this lack of time seemed to result in a disconnect between pedagogy and technology 

 

One participant described what he felt were mixed messages from his institution about preferred forms of assessment supported by technology 

 

This comment neatly summarizes the conundrum faced by many higher education teachers as they try to adopt new approaches while also designing appropriate and efficient forms of assessment for students. In summary, our participants variously regarded technology as modern, challenging, innovative, imperfect, and inevitable. 

 

Theme 3: Technology-based assessment designs aimed to shape, and were shaped by, student behavior 

 

Encouraging particular student behaviors was also a significant driver for technology-supported assessment. This included providing opportunities for students to self-test their understanding through online quizzes, which could free time in tutorial or practical classes for more effective forms of teachingand learningor toallowfor targetedremedial support 

 

One common approach involved weekly online quizzes for nominal marks to motivate students to complete readings 

 

There was a belief amongst participants that students expected and welcomed these forms of assessment, particularly online quizzes. 

 

Others also reflected on the challenge of rewarding participation through appropriate credit for online activities, while at the same time acknowledging that collusion meant it was impossible to be confident students had submitted their own work. The compromise was generally that online activities received a small proportion of the marks. 

 

Our participants also expressed concerns with respect to students’ technological access and ability. One interviewee described feelingrestrictedin what she coulddesign becauseof limitations in students’ technical skills 

 

Others explained that they were unable to take full advantage of online quizzes for assessment because not all students had access 

 

Taken together, the examples above demonstrate how new assessment designs were created or adjusted in response to student behaviors; for example, to combat a lack of student engagement, encourage self-directed learning and mitigate the risks of cheating or inequitable access. 

 

Theme 4: Implementing technology-supported assessment requires support and compromise 

 

Participants consistently identified inadequate support as a major challenge to their efforts to integrate technology into assessment. This was often exacerbated by their own inability to communicate effectively with technically oriented support staff 

 

Inadequate infrastructure also posed barriers to participants achieving everything they had hoped. In some cases, online tools could not be used to hold invigilated online exams due to limited computer lab space and students’ lack of access to specialist software. 

 

Interviewees also highlighted the need to overcome other logistical hurdles associated with technology-supporteddesigns.Thisinvolved bothanticipatingchallengeswhencreatinga design and adapting a design iteratively over several implementations to improve it. Participants described the need to find a way to make their ideas work using the technology tools available, often resulting in “work-arounds” and compromises. The risks of this kind of experimentation were high 

 

The “state of the actual” is a complex array of barriers and enablers that give rise to inconsistent adoption. Dramatic increases in university enrolments over the past decades have led to increasing pressure to efficiently assess large numbers of students, while also providing high-quality educational experiences and engaging in innovative practice (Nicol, 2010). 

 

Recent studies have explored how technology can reduce marking time and administration, automate feedback, improve students’ engagement with feedbackand offer newopportunitiesfor formativeassessment(Atkinson& Lim, 2013; Daly,Pachler, Mor & Mellar, 2010; Nix & Wylie, 2011; Snodgrass, Ashby, Rivett & Russell, 2014). In this con- text, technology presents both solutions and challenges. 

 

Part of the complexity indicated in our findings is that barriers and enablers are variable and context-dependent. This is not surprising, given that some forms of assessment are more appropriate or acceptable in some disciplines than others, some institutions are better resourced in terms of technology, some have more skilled support staff and some are less bureaucratic in technology policy and management (Theme 4: Support and compromise). 

 

Two experiences stand out as common across participants, however.One was negotiating the tensio 

 

between havingto generate efficiencies in assessment while also implementing innovative pedagogies (Theme 1: Economics of assessment). While the dominant form of technologyfacilitated assessment emerged as online quizzes, this was deemed pedagogically satisfactory rather than optimal. Many participants acknowledged their institution’s goals and expressed an interest in developing new approaches using technology, but did not always feel capable of responding (Theme 4: Support and compromise). This is consistent with studies of e-learning adoption more generally (eg, Kirkwood & Price, 2013). 

 

Another common experience was of a lack of time (Theme 1: Economics of assessment). Our participants variously mentioned lacking time to collaborate, solve technical and logistical problems, learn new skills or consult others. All of these affected their capacity to integrate technology into assessment. Technology adoption requires a commitment to learning new tools, but also access to good information about possibilities and appropriate support (King & Boyatt, 2014) (Theme 4: Support and compromise). 

 

Drawing from a socio-material perspective (Fenwick, Edwards & Sawchuk, 2011), the data strongly suggests that technology-supported assessment designs are the product of a dynamic relationship between the academic, the technological tool and the broader context. For example, several accounts of new approaches were clearly led by the functionality offered by the tool, and this was particularly true of tools embedded in learning management systems. That is, the specific uses of online quizzes, wikis or marking rubrics were a consequence of their avail- ability, the teacher’s desire to “do something new” and the broader institutional approach to technology in education (Theme 2: Contemporary and innovative). Further, when academics focused on pedagogicalconsiderations,theyoftenexperiencedchallengesbecause thetools availablewere eithernotcapableof ornot configuredfor theirdesign (Theme4:Supportand compromise). 

 

Our findings also highlight the “romance” associated with adopting “cutting edge” technologysupported methods of teaching to demonstrate currency and teachers’ capacity to take risks (Theme 2: Contemporary and innovative). 

 

Participants’ experiences of teaching with their technology-supported assessments help us to understand why some designs do not work in practice. The chief problems arose when new designs introduced unanticipated inefficiencies, particularly when marking proved more timeconsuming than expected, or when an approach did not shape students’ behaviors in the way 

 

intended(Themes 1 and 3). 

 

Overall, these implications suggest that approaching assessment design as a process of formative development over multiple iterations could be greatly beneficial. This would lower the stakes at the beginning, enable a gradual roll-out over time, anticipate opportunities to gather evidence and reflect, and help to manage workloads and resourcing. 

 




%% Import Date: 2023-08-03T06:33:14.465-07:00 %%

---
DOI: 
Date: 2019-01-01T00:00:00-08:00
Rating: 0/5
Title: Educational and psychological measurement
ShortSummary: 
annotation-target: finchEducationalPsychologicalMeasurement2019.pdf
tags: 
---


#### [Educational and psychological measurement](finchEducationalPsychologicalMeasurement2019.pdf)


> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Finch, W. H., & French, B. F. (2019). _Educational and psychological measurement_. Routledge.

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 




#### Annotations


Standards for Educational and Psychological Testing 

 

Tis notion of using samples to represent populations lies at the heart of much of statistical analysis and will be a core idea that we will rely upon throughout this book. Of key importance in the whole sampling enterprise is the need to make the sample representative of the population, which is most easily accomplished through random selection of individuals from the population of interest. 

 

 ( EF 2 ) = COV IQ 3 , ( 1 7 . 

[[Why is there no COV(EF1,EF2)=...]] 

2 [ 3 8 ] . + + 5 2 4 7 + . . 

[[Why is this doubled?]] 

At the heart of CTT is the idea of the true score for the construct that we are measuring (e.g., math achievement), and its relationship with the observed score on a test 

 

the observed test score, the true score for an individual on the tested material, and random variability in the observed score caused by factors other than the true ability (e.g., fatigue, distractedness). 

 

We will resist calling this the true score model as this would imply that it can be falsifed, which it cannot (e.g., Raykov & Marcoulides, 2011; Zimmerman, 1975). In other words, the relationship between the observed score, true score, and error can be conceived of conceptually, but cannot be formally tested using observed data. 

 

Indeed, the key to understanding the concept of the true score framework is to frst understand error and its central role in all of measurement. We will discuss the two primary types of measurement error: (1) random error, which is specifc to a time, place, examinee, and assessment, and balances out across these four factors; and (2) systematic error, which has a consistent impact across one or more of time, place, examinee, and assessment, and leads to biased, upward or downward, observed score values. 

 

the parallel forms model, 

 

tau-equivalent assessments 

 

essentially tau-equivalent assessment 

 

congeneric assessments 

 

Tese other factors infuencing test performance are lumped together under the umbrella of error. 

 

In other words, a shorthand way for us to think about error is that it represents everything that might infuence the math score, other than actual math ability, T. M 

 

Random error refers to factors that are transient and idiosyncratic in nature, afecting a single individual’s performance on the scale only at the moment in time at which they complete the instrument. Random error is unique from time to time and from person to person, meaning that if an individual were to be given the math test at multiple points in time, the error would difer each time. 

 

Random error is unrelated to T, but does impact the value of X. 

 

Practically speaking, this means that the impact of random error on test scores is not more (or less) for individuals with higher math ability (i.e., larger values of T). In addition, the impact of random error on X would not be consistent were the student to take the test repeatedly, and because random error is idiosyncratic to an individual person, it is not related to the random error associated with other individuals who take the test. Finally, given the fact that it is transient across people and time, random error does not lead to biased estimation of the true score of interest, either within an individual or across individuals, as the random values will tend to cancel each other out. What this means in practice is that the mean value of random error taken across multiple test takers is 0. 

 

Unlike random error, systematic error has a consistent impact on the value of X and would yield essentially the same infuence on the observed scale score were an individual to be administered the instrument repeatedly. 

 

For example, if the math test described above contains grammatical errors or unclear wording, this may lead to confusion and incorrect responses for a large number of the examinees. 

 

In each example, the error is not associated with the construct being measured (math) but does have a consistent impact on the observed scale scores across people and time. 

 

However, it is also possible for us to conceive of tests as being individual subscales within a battery of assessments, such as with intelligence tests. In addition, individual items can also be thought of as very brief and specifc 

 

assessments of the construct of interest. 

 

It is important to remember, however, that though these models may be more complex in form, they represent the same basic links between observed scores, true scores, and error. 

 

= + X ij T ij E ij (Equation 3.2) 

 

Tis value, which is commonly referred to as reliability, can be interpreted as the proportion of observed score variance accounted for by the true score. 

 

In each of these situations, we want to quantify the extent to which the raters agree (or do not agree) with one another on their scores. 

 

Our defnition of reliability focused on the ratio of true score variance to total scale variance (the sum of true score and error variances), such that higher levels of reliability were associated with relatively less variation due to measurement error. 

 

 

 

In this view, validity statements cannot be made about an instrument, but rather must refer to inferences that are made using scores from the scale. Furthermore, various types of validity evidence are not compartmentalized (i.e., criterion validity, content validity) as was the case in earlier years, but rather are used in conjunction with one another in order to build an argument for the extent to which valid inferences and uses can be made with scores derived from the instrument. 

 

Eventually, construct validity came to be understood by many psychometricians as an umbrella term covering all aspects of validity assessment. In other words, each of the various approaches to understanding validity were, in fact, just diferent aspects of an attempt to fully understand the degree to which an instrument truly measured the construct of interest. 

 

. Specifcally, Messick stated that rather than validating a scale, psychometricians are actually validating inferences drawn from scores produced by the scale. 

 

Te importance of Messick’s contribution to validity theory and practice cannot be overstated. His arguments in favor of focusing on the validity of score inference rather than on the scale itself is the centerpiece of what could reasonably be argued to be the ascendant view of validity today. 

 

It is difcult to imagine a scenario in which the items comprising a scale are poor, but the scale itself is of high quality. Terefore, we need to understand something about how well individual items work in order to know how well the scale itself may work. 

 

Furthermore, when evidence for reliability and validity yield a mixed picture regarding scale quality, item analysis can provide the researcher with information that might help diagnose causes for such problems. 

 

Rather, the simpler methods introduced in this chapter can function in concert with more complex techniques, such as IRT, to provide measurement professionals and others with a full picture of an individual item’s qualities. In addition, for individuals working with small samples, the methods that we discuss in Chapter 10 will be preferable to IRT because they do not require large numbers of examinees in order to obtain trustworthy or accurate estimates. 

 

We learned that items can be characterized in terms of their ability to diferentiate among individuals with diferent scores on the full scale (item discrimination), as well as in terms of the likelihood of respondent endorsement (item difculty). 

 

Collectively, these methods are known as item response theory (IRT), and in many ways they form the backbone of modern analytic approaches to understanding the behavior of individual test items. 

 

A primary advantage of IRT over the classical test theory-based approaches for calculating item statistics that we described in Chapter 10 is that the estimation of item and person statistics are independent of one another, so that we can interpret statistics such as item difculty independently of the sample from which it was estimated, and we can interpret person location independently of the items used to estimate it. 

 

Te ICC expresses the relationship between the item response (e.g., correct or incorrect), and the latent trait being measured (e.g., reading achievement). 

 

Te ICC also includes information about the item as well, such as difculty, discrimination, and the probability of a correct response due to chance. Each of these dichotomous IRT models can be represented using an ICC. 

 

deviations or more above average have an 80% chance of answering the item correctly, based on the Rasch model results. 

 

Once again, we can estimate the probability of a correct item response by connecting the xand y-axes through the ICC. For the 3PL model of our example item, an individual with history knowledge that falls 2 standard deviations below average (−2) has approximately a 42% chance of correctly answering the item correctly (dashed lines in Figure 11.3). 

 

Te equation for the Rasch model, expressing the relationship between the probability of item endorsement and the latent trait is written as follows: 

 

Where xj = Response to item j by individual i bj =Location (e.g., difculty) for item j θi = Level of the latent trait (e.g., ability) for individual i. 

 

One of the signal traits of the Rasch model is that the item discrimination parameter is set to 1 for all items. Recall that item discrimination refers to the ability of an item to diferentiate individuals with diferent levels of the latent trait being measured by the scale. 

 

Discrimination is also a measure of the strength of relationship between the item response and the latent trait 

 

Equation (11.1) is really just a special case of equation (11.2), where a = 1 for all items. Indeed, estimates from the Rasch and 1PL can be easily converted from one to the other, though we do not show that here as it is not germane to understanding how these models can be used in practice. This correspondence between estimates from the two models is important for us to mention, however, because similar equivalences are not possible when discussing the more complex IRT models that come next. 

 

Te assumption of equal discrimination values across items is very strong and may be unreasonable in many (perhaps most) cases. When this assumption is not tenable, a researcher can relax it and allow for separate item discrimination values for each item. Te model that relaxes this equivalent discrimination assumption is the 2-parameter logistic (2PL) model, 

 

As mentioned before, items with larger a values are more desirable because they are more closely tied to the latent trait, and thus provide more information about individuals at diferent levels of the underlying construct being measured. We should note two important points here. First, relaxing the equal item discrimination condition with the 2PL model will impact the estimation of item difculty and person ability. Tus, those values will be somewhat diferent than what we saw for the Rasch and 1PL models. Indeed, we saw this 

 

phenomenon in action when we allowed the estimation of the common discrimination parameter, as item difculties difered between the Rasch and 1PL results. Second, we will continue to constrain the pseudo-chance parameter to be 0 with the 2PL model. 

 

As we alluded to earlier, the 3-parameter logistic (3PL) model is very similar to the 2PL, with the addition of the pseudo-chance or guessing parameter (c), expressing the probability that an individual will answer an item correctly due solely to chance. 

 

Tus, only when an individual could realistically endorse an item due solely to chance would we want to use the 3PL model. 

 

Items that provide a great deal of information about an individual’s ability level (e.g., history knowledge) are preferable to those that do not provide much information in this regard. 

 

From these results, we can see that, all things being equal, information is greater for items with larger discrimination parameter values. 

 

It is, therefore, of paramount importance that items on such instruments, particularly in the context of high-stakes testing, be assessed for the presence of DIF in order to ensure that they are providing appropriate information about all respondents. 

 

Again, such designations are arbitrary, but typically the focal group is the one that we expect DIF to impact negatively in terms of their item responses. 

 

Te subject of test and item bias is fraught with social and educational controversy. It is important to note at the outset that DIF and bias are not the same thing. DIF detection methods can be used to identify items that may be biased, but the presence of DIF alone does not mean that an item is biased, as we discuss below. Tis is a crucial point to remember in our conversation moving forward. 

 

In general, when we say that an item demonstrates bias we are saying that there exists a systematic process, other than the construct being measured, that infuences the response to the item for some examinees but not for others. 

 

Te important point here is that DIF refers to a statistical diference in the probabilities of a specifc (e.g., correct) item response between persons in diferent groups but with equal levels of the latent trait being measured. In contrast, item bias refers not only to the diference in these item response probabilities, but also to the determination as to why an item functions diferentially. In other words, DIF is limited to the results of a statistical analysis about conditional group diferences in item responses, whereas bias refers to the combination of such statistical results with an explanation as to the source of group diferences on an item (Angof; Camilli & Shepard, 1994). T 

 




#### Related
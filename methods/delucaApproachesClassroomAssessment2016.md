---
DOI: 10/gfgtsg
Date: "2016"
Rating: 0/5
Title: Approaches to classroom assessment inventory A new instrument to support teacher assessment literacy
annotation-target: delucaApproachesClassroomAssessment2016.pdf
tags:
  - pca
  - expert-panel
  - pearsons-chi-square
  - kaiser-meyer-olkin
  - coefficient-alpha
---


#### [Approaches to classroom assessment inventory: A new instrument to support teacher assessment literacy](delucaApproachesClassroomAssessment2016.pdf)


> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>DeLuca, C., LaPointe-McEwan, D., & Luhanga, U. (2016). Approaches to classroom assessment inventory: A new instrument to support teacher assessment literacy. _Educational Assessment_, _21_, 248–266. [https://doi.org/10/gfgtsg](https://doi.org/10/gfgtsg)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 




#### Annotations


Specifically, assessment literacy has been defined as the skills and knowledge teachers require to measure and support student learning through assessment (Brookhart, 2011; Popham, 2013). Researchers have constructed instruments to measure and track teachers’ assessment literacy in an effort to support teachers’ classroom assessment practices. However, based on their recent systematic review of 36 literacy measures, Gotch and French (2014) found that there is weak psychometric evidence supporting these measures and that existing instruments lack “representativeness and relevance of content in light of transformations in the assessment landscape (e.g., accountability systems, conceptions of formative assessment)” (p. 17). 

 

These findings are not surprising given that the majority of assessment literacy measures are predicated on assessment standards from the early 1990s (i.e., Standards for Teacher Competency in Educational Assessment of Students; American Federation of Teachers [AFT], National Council on Measurement in Education [NCME], & National Education Association [NEA], 1990; DeLuca, LaPointe-McEwan, & Luhanga, 2016; Gotch & French, 2014). 

 

Although the 1990 assessment standards were useful in supporting teacher learning and describing valuable assessment practices, Brookhart (2011) recognized that the standards no longer fully account for the range of assessment activities or the assessment knowledge teachers require within the current landscape of schooling. In particular, Brookhart noted that the standards need to be revised in recognition of three dominant shifts: (a) the increased reliance on assessments for accountability purposes, including large-scale and standardized measures administered within classroom contexts and related to teacher instruction and evaluation; (b) formative assessment activities, including 

 

contemporary conceptions of assessment for and as learning; and (c) the enactment of assessments within and in response to highly diverse (i.e., sociocultural, linguistic, ability, etc.) teaching contexts. 

 

We further add the emerging trend toward assessment education—a deliberate focus within professional learning communities to enhance and support teachers’ assessment practices and data literacy (DeLuca et al., 2016). Combined, the absence of these core aspects of current assessment practices diminishes construct representation within existing assessment literacy measures and limits the validity of inferences drawn about teachers’ knowledge and skills (Brookhart, 2011; DeLuca et al., 2016; Gotch & French, 2014). 

 

Given the significant shifts in the assessment landscape over the past 20 years, in this article we aim to construct a reliable instrument reflective of current assessment practices and contexts. Specifically, we draw on the recently developed Classroom Assessment Standards (Joint Committee on Standards for Educational Evaluation [JCSEE], 2015) as a basis for delineating teachers’ current assessment practices. Based on our analysis of those and other assessment documents, we present our process for developing a new assessment literacy measure including construct validity testing and initial psychometric evidence drawn from a sample of more than 400 teachers. We conclude by discussing the potential uses of this assessment literacy instrument for teachers, professional learning, and future research. 

 

Previous measures of teacher assessment literacy 

 

In their seminal study, Plake, Impara, and Fager (1993) examined the assessment competency of 555 teachers and 268 administrators across 45 U.S. states. In the first part of their study, a 35-item instrument, the Teacher Competencies Assessment Questionnaire (TCAQ), was developed to measure the seven competency standards. Each standard was measured through five multiple-choice items. 

 

Building on Plake et al.’s(1993) research, O’Sullivan and Johnson (1993) used the TCAQ with 51 graduate students enrolled in a measurement course for teachers. 

 

Similarly, Campbell, Murphy, and Holt (2002) administered a revised version of the TCAQ to 220 undergraduate students enrolled in a preservice measurement course. 

 

Mertler (2003) identified a similar trend when he administered the TCAQ to 67 preservice and 197 practicing teachers. Mertler’s results paralleled Plake et al.’s(1993) and Campbell et al.’s studies. 

 

In 2004, Mertler and Campbell collaborated on reconceptualizing the TCAQ into the Assessment Literacy Inventory (ALI; Mertler & Campbell, 2004, 2005). Their aim was to contextualize the items by restructuring them into scenario-based questions, reflecting a more practical orientation to the 

 

Standards (AFT, NCME, & NEA, 1990). The ALI included seven scenarios, each linked to one of the standards, with a series of five multiple-choice questions. 

 

Brown (2004) and his later work with colleagues (e.g., Brown & Harris, 2009; Brown & Hirschfeld, 2008; Brown, Hui, Flora, & Kennedy, 2011) used the Teachers’ Conceptions of Assessment (COA) questionnaire to determine teachers’ priorities related to four purposes of assessment: (a) improvement of teaching and learning, (b) school accountability, (c) student accountability, and (d) treating assessment as irrelevant. On this instrument, teachers were asked if they agreed or disagreed with various assessment purposes related to these four conceptions. 

 

Across these studies, the COA is used to define teachers primary priorities related to the purposes of assessment with consideration for their values toward assessment practices. The findings from these studies point to a variability in teachers’ conceptions of assessment depending upon context and career stage. 

 

Overall, the majority of research on teacher assessment competency employs instruments that aim to determine teachers’ orientation toward varying assessment purposes. Results from these studies continue to characterize teachers’ assessment competency as largely incongruent with the recommended 1990 Standards (Galluzzo, 2005; Mertler, 2003, 2009; Zhang & Burry-Stock, 1997). 

 

In particular, an instrument is required that addresses the contemporary demands on teachers working within the current accountability framework of education and that accounts for the multiple dimensions of assessment literacy beyond solely addressing assessment purposes. 

 

Instrument development process 

 

We used a multistep development process to construct a comprehensive assessment literacy instrument reflective of current assessment standards. Specifically, we (a) engaged in a document analysis of previous and current assessment standards to assist with initial item development and (b) collected validity evidence to support the intended interpretations and uses of the instrument. Of the five sources of validity evidence (i.e., content, response processes, internal structure, relationship to other variables, and consequences) described in the 2014 Standards for Educational and Psychological Testing (American Educational Research Association, American Psychological Association, & NCME, 2014), this article presents evidence of validity based on content and internal structure. 

 

Initial item development 

 

Our instrument development process began with a document analysis of 15 assessment standards (1990 – present) from six geographic regions (United States, Canada, United Kingdom, Europe, Australia, and New Zealand; see DeLuca et al., 2016, for complete analysis of standards). 

 

Through this analysis, we identified eight themes representing contemporary aspects of teacher assessment literacy: (a) Assessment Purposes, (b) Assessment Processes, (c) Communication of Assessment Results, (d) Assessment Fairness, (e) Assessment Ethics, (f) Measurement Theory, (g) Assessment for Learning, and (h) Assessment Education and Support for Teachers. For the purpose of developing a reliable yet efficient instrument, we collapsed seven of the themes into four: (a) Assessment Purposes (included 

 

aspects of assessment for learning), (b) Assessment Processes (included aspects of communication of assessment results), (c) Assessment Fairness (included aspects of assessment ethics), and (d) Measurement Theory (Table 1). 

 

Questions related to these themes were presented in Part 1 and Part 2 of our instrument. The eighth theme, Assessment Education and Support for Teachers, was directly targeted in Part 3 of the instrument and underpins the overarching purpose of this instrument development—to develop an assessment literacy instrument to measure and support teachers’ professional learning and practice in classroom assessment. 

 

We purposefully elected to use terminology related to teachers “approaches to classroom assessment” for the ACAI rather than “assessment literacy” or “assessment competency” for two primary reasons. First, we wanted to ensure that the instrument remained accessible to a wide teacher population and was not jargonistic. Second, “approaches to classroom assessment” reflects the multiple perspectives and practices that teachers might hold in relation to classroom assessment, whereas “assessment literacy” and “assessment competency” have been associated with correct and incorrect assessment knowledge and skills with evaluative underpinnings (Willis, Adie, & Klenowski, 2013). 

 

Part 1: Approaches to Classroom Assessment 

 

Part 1 of the ACAI was designed to determine teachers’ approaches to classroom assessment. Following Mertler and Campbell (2004, 2005), we used a scenario item format; however, unlike previous instruments that presented a multiple-choice correct answer format, each response to our scenarios was viable and reflected differing approaches to classroom assessment as based on assessment standards. 

 

The five scenarios represented contemporary assessment dilemmas faced by teachers related to (a) summative assessment, (b) grading, (c) differentiated assessment, (c) integrated assessment, and (d) standardized assessment. For each scenario, we developed four items in relation to the four assessment literacy themes (i.e., Assessment Purposes, Assessment Processes, Assessment Fairness, and Measurement Theory). Each item had three viable response options that reflected different approaches to classroom assessment (see Table 2). We purposefully developed categorical approaches in keeping with previous research that conceptualized teacher practice in relation to categories and groups (e.g., Brown, 2004; Trigwell & Prosser, 2004). 

 

Approach A focuses on summative approaches to assessment that emphasize reliable assessment design and the standardized administration of assessments of learning across groups. Approach B focuses on formative approaches to assessment that emphasize validity through emphasis on use and scoring of assessments of learning and equitable treatment of students. Approach C focuses on 

 

individualized approaches to assessment that emphasize both reliability and validity concerns through communication and a differentiated approach to assessments through assessments as learning. 

 

Part 3: Assessment Professional Learning Priorities and Preferences 

 

Evidence of validity based on content: Expert-panel review 

 

An expert-panel method was used to collect construct validity evidence for the ACAI. Invitations to review the assessment literacy instrument were e-mailed to 24 North American educational assessment experts. Experts were recruited based on the following criteria: (a) membership in the Canadian Education Researchers’ Association or NCME; (b) published in the area of classroom or large-scale assessment; and/or (c) an active member of a university, school district, or education/assessment-related institution. Ten experts meeting these criteria agreed to participate. In addition to an educational assessment expert panel, we invited 10 classroom teachers (five elementary and five secondary) to serve as a practitioner expert panel. All panelists reviewed the instrument using the same protocol. 

 

Each expert participant was sent the assessment literacy instrument with review instructions via e-mail. An alignment methodology was used to guide the expert review process (DeLuca & Bellara, 2013; Webb, 1997, 1999, 2005). For Part 1: Approaches to Classroom Assessment, experts were asked to double rate each response option based on its alignment to (a) the identified assessment literacy theme and (b) the identified assessment priority (see Tables 3 – 7). A rating scale from 1 (not aligned)to5 (strongly aligned) was used for alignment ratings. For items with overall average ratings of 3 or less, experts were asked to (a) explain their rating rationale and (b) suggest alternative response options. In addition to alignment ratings, experts were asked to suggest revisions to the five assessment scenarios that comprised Part 1 of the instrument. 

 

For Part 2: Confidence in Assessment, experts were asked to indicate whether each item reflected the complexity, depth, and range (DeLuca & Bellara, 2013) of the associated theme (i.e., yes or no) and to suggest revisions as appropriate. 

 

For Part 3 (A): Assessment Professional Learning Priorities, experts were asked if the items represented the complexity, depth, and range of assessment competency (i.e., yes or no) and instructed to suggest revisions as appropriate. 

 

For Part 3 (B): Assessment Professional Learning Preferences, experts were asked to suggest additional modes of professional learning that were not included in the draft instrument. 

 

Experts were asked to participate in up to three rounds of review, until all items in Part 1 received overall average ratings of 4 or 5 and all items in Parts 2 and 3 received complete yes responses. Once all items meet these criteria, the instrument was pilot tested with more than 400 teachers. 

 

Expert-panel results 

 

Based on feedback from experts, several revisions were made to the assessment literacy instrument. 

 

Evidence based on internal structure: Pilot testing 

 

Sample A survey method was used to collect validity evidence based on internal structure. Participants completed the ACAI through an electronic invitation posted on various social and media networks. The sample consisted of 404 Canadian teachers who completed the ACAI in spring 2015. 

 

categorical nature of item response options, 

 

mode statistics were calculated as measures of central tendency across scenarios 

 

categorical nature of response options, traditional correlation-based internal consistency estimates were inappropriate measures of internal structure 

 

Pearson’s chi-square tests were used to examine whether response patterns (mode statistics) were consistent across demographic groupings #pearsons-chi-square 

 

Principal component analysis with promax (oblique) rotation was used to generate subscales out of the 26 items #pca 

 

The KaiserMeyer-Olkin measure was used to verify the sampling adequacy for the analysis. #kaiser-meyer-olkin 

 

Cronbach’s alpha #coefficient-alpha 

 

Resulting subscale means and standard deviations were then calculated, and Cronbach’s alpha was used to calculate the internal consistency of each resulting subscale. 

 

frequency distributions 

 

modal value 

 

Pearson’s chi-square tests #pearsons-chi-square 

 

Frequency Distributions 

 

Principal component analysis 

 

principal component analysis 

 

Supporting data-informed professional learning in classroom assessment. Research is needed on how the ACAI, or other indicators of teacher assessment literacy, could be used to support the selection and monitoring of teacher learning goals related to classroom assessment. For example, individual and systemic data collected through the ACAI could provoke collaborative inquiries for teacher learning or support school and district professional learning initiatives (e.g., workshops, expert support, resources). We assert that there is value in future research that tracks how teachers use data about their own assessment literacy to guide their professional learning in assessment, and how regions and districts use data on teachers’ approaches to assessment to direct systemic professional development resources 

 

as a developmental process that is mediated by context, opportunity to learn, personal preferences, and school/system culture. 

 

Specifically, assessment literacy has been defined as the skills and knowledge teachers require to measure and support student learning through assessment (Brookhart, 2011; Popham, 2013). Researchers have constructed instruments to measure and track teachers’ assessment literacy in an effort to support teachers’ classroom assessment practices. However, based on their recent systematic review of 36 literacy measures, Gotch and French (2014) found that there is weak psychometric evidence supporting these measures and that existing instruments lack “representativeness and relevance of content in light of transformations in the assessment landscape (e.g., accountability systems, conceptions of formative assessment)” (p. 17). 

 

We purposefully elected to use terminology related to teachers “approaches to classroom assessment” for the ACAI rather than “assessment literacy” or “assessment competency” for two primary reasons. First, we wanted to ensure that the instrument remained accessible to a wide teacher population and was not jargonistic. Second, “approaches to classroom assessment” reflects the multiple perspectives and practices that teachers might hold in relation to classroom assessment, whereas “assessment literacy” and “assessment competency” have been associated with correct and incorrect assessment knowledge and skills with evaluative underpinnings (Willis, Adie, & Klenowski, 2013). 

 

Supporting data-informed professional learning in classroom assessment. Research is needed on how the ACAI, or other indicators of teacher assessment literacy, could be used to support the selection and monitoring of teacher learning goals related to classroom assessment. For example, individual and systemic data collected through the ACAI could provoke collaborative inquiries for teacher learning or support school and district professional learning initiatives (e.g., workshops, expert support, resources). We assert that there is value in future research that tracks how teachers use data about their own assessment literacy to guide their professional learning in assessment, and how regions and districts use data on teachers’ approaches to assessment to direct systemic professional development resources. 

 




#### Related
---
DOI: 10/ghdnhh
Date: 2010
Rating: 0/5
Title: "Assessment, Technology, and Change"
ShortSummary: ""
annotation-target: clarke-miduraAssessmentTechnologyChange2010.pdf
---


#### [Assessment, Technology, and Change](clarke-miduraAssessmentTechnologyChange2010.pdf)




> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Clarke-Midura, J., & Dede, C. (2010). Assessment, Technology, and Change. _Journal of Research on Technology in Education_, _42_(3), 309–328. [https://doi.org/10/ghdnhh](https://doi.org/10/ghdnhh)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 


#### Related

#### Annotations


However, using technology to deliver automated versions of item-based paper-and-pencil tests does not realize the full power of ICT to innovate via providing richer observations on student learning. 

 

Such an advance in assessment is greatly needed because current methods of testing are incapable of validly measuring sophisticated intellectual and psychosocial performances. For example, studies of national tests, such as the (NAEP), showed the items related to scientific inquiry did not align with the inquiry content they were supposed to be measuring (Quelmalz & Haertel, 2004; Haertel, Lash, Javitz, & Quellmalz 2006; Quellmalz, Kreikemeier, DeBarger, & Haertel, 2007). These studies recommended that test designers redesign some of their items and integrate technology-based measures. But why are item-based, paper-and-pencil tests inadequate for important student outcomes, such as scientific inquiry and 21st century skills? 

 

We cannot directly inspect what students know or do not know. Like Sherlock Holmes solving mysteries, assessment involves indirect reasoning from evidence: developing a model of cognition reflecting the knowledge a learner is to master, collecting observations of a student’s statements and behaviors, and interpreting the extent to which those statements and behaviors match the expert model. Over the last few decades, cognitive scientists have greatly increased the depth and validity of their models, and psychometricians have made major advances in interpretive analytics. However, the observation part of the triangle, centered on paper-and-pencil item-based tests, has remained weak for about a century,. These tests cannot generate a rich range of observations: a student’s forced choice among a few predetermined options is a weak observation on whether they have mastered a sophisticated skill involving advanced knowledge. Without detailed observations that document every aspect of a learner’s performance, little is available to compare to the highly specified cognitive model using advanced interpretive analytics. Attempts to improve assessment have repeatedly foundered on this problem of impoverished observations of student performances within the rigorous conditions required to ensure the fairness, reliability, and validity of sophisticated intellectual assessments. Our research is attempting a breakthrough in assessment because technical capabilities now exist for providing rich observations about student learning. 

 

However, advances in technology and statistics are creating new possibilities and promises for assessment. The type of observations and evidence of learning that technologybased assessment allow for is unparalleled. 

 

Paper and pencil tests are barely adequate to measure minimum competencies required for low-level roles in industrial settings and fall woefully short of providing measures of the sophisticated knowledge and skills students need for 21st century work and citizenship. 

 

States’ high-stakes psychometric tests are typically based on multiple-choice and short-answer items that have no mechanism for assessing attainment of higher order understandings and performances (National Research Council, 2001). As a result, the curriculum is crowded with low-level facts and recipe-like procedures (e.g., In what year did Columbus discover America? What are the seven steps of historical inquiry?), as opposed to nuanced understandings and performances (i.e., What confluence of technological, economic, and political forces led to the age of exploration around the end of the 15th century? 

 

Because of the accountability systems linked to students’ performance on these high stakes tests, teachers are using weak but rapid instructional methods, such as lecture and drilland-practice, to race through the glut of recipes, facts, and test-taking skills they are expected to cover. Despite research indicating that guided inquiry, collaborative learning, mentoring, and 

 

apprenticeships are far more effective pedagogical strategies, introducing these into school settings is difficult given the crowded curriculum and the need to prepare students for high stakes tests. 

 

Teachers have no means by which to prioritize what understandings and performances to emphasize in terms of 21st century citizenship; workplace capabilities for the global, knowledge-based economy; and lifelong learning (Dede, 2007; Dede, in press). Nor are students’ abilities to transfer their skills to real world situations are assessed. 

 

In addition, policies such as financial incentives for teachers and districts to raise test scores can exacerbate already troubling differences in educational outcomes, promoting the abandonment of the very at-risk students the NCLB legislation was intended to help (Confrey & Maker, 2005). While modern interactive media could aid with these shortfall of current high stakes tests, the use of ICT applications and representations is generally banned from testing. Rather than measuring students’ capacities to use tools, applications, and media effectively, various forms of mediated interaction are typically not assessed. In other words, the effects from technology usage (what one can accomplish without tools) are measured, but the effects with technologies essential to effective practice of a skill are not (Salomon, 1993). 

 

In the 1990s there was a movement towards developing alternate assessments in science education that measured students’ conceptual understanding and higher level skills, like problem solving (Linn, 1994). Numerous studies were conducted to assess the reliability and construct validity of these performance assessments and also the feasibility (i.e., cost effectiveness and practicality) of using them large scale (Linn, 2000). 

 

While research supports performance tasks as valuable both for aiding learning and for providing formative, diagnostic feedback to teachers about ongoing student attainment, when used as summative assessments performance tasks were 

 

ound to be not as cost-effective as multiple choice tests (Stecher & Klein, 1997). Also, performance assessments had troubling issues around task sampling variability (Shavelson, Baxtor, and Gao, 1993). 

 

In summary, the goal of an assessment is to provide valid inferences related to particular expectations for students (Linn et al, 2002). While an assessment can serve multiple purposes, it is not possible for one assessment to meet all purposes; for example, an assessment providing information about students’ deep conceptual understanding that can be used by educators to guide instruction would be different from an assessment that provides an evaluation of an educational system for policymakers (NRC, 2001). 

 

The most prevalent current use of technology in large-scale testing involves support for assessment logistics related to online delivery, automated scoring, and accessibility. 

 

However, research on next generation assessments is breaking the mold of traditional testing practices. Innovative assessment formats, such as simulations, are being designed to measure complex knowledge and inquiry previously impossible to test in paper-based or handson formats. These new assessments aim to align summative assessment more directly to the processes and contexts of learning and instruction (Quellmalz & Pellegrino, 2009). 

 

In addition to citing the importance of assessments related to inquiry, the experts involved suggest that interactive computer tasks be introduced to assess students’ knowledge, skills, and abilities related to the following situations (National Assessment Governing Board, 2007, page 107): 

 

• For scientific phenomena that cannot easily be observed in real time, such as seeing things in slow-motion (e.g., the motion of a wave) or speeded-up (e.g., erosion caused by a river). It is also useful when it is necessary to freeze action or replay it. • For modeling scientific phenomena that are invisible to the naked eye (e.g., the movement of molecules in a gas). • For working safely in lab-like simulations that would otherwise be hazardous (e.g., using dangerous chemicals) or messy in an assessment situation. • For situations that require several repetitions of an experiment in limited assessment time, while varying the parameters (e.g., rolling a ball down a slope while varying the mass, the angle of inclination, or the coefficient of friction of the surface). • For searching the Internet and resource documents that provide high-fidelity situations related to the actual world in which such performances are likely to be observed. • For manipulating objects in a facile manner, such as moving concept terms in a concept map. 

 

The troubling findings about performance assessments are largely due to the intrinsic constraints of paper-based measures, coupled with the limited capabilities of virtual assessments created based on what computers and telecommunications could accomplish a decade ago when research on performance assessments in accountability settings flourished. 

 

Assessments developed with current, more sophisticated immersive technologies face fewer threats from generalizability and efficiency concerns than traditional performance assessments. 

 

In addition, because virtual situations are more easily uniformly replicated, virtual performance assessments may not encounter the significant error from occasion. As mentioned above, studies on traditional performance assessments found that student performances varied 

 

one on occasion to another (Cronbach et. al., 1997) and that the occasion of the sample was compounded with task-sampling (Shavelson et al, 1999). Developing virtual assessments can be more cost-effective for schools, easier to administer and score, and can address task and occasion sampling variability through design. For example, as opposed to kits of tasks that contain items and objects, virtual performance assessments enable automated and standardized delivery via a web-based application. By making the sub-tasks independent, one can design assessments with a larger number of tasks, thus increasing the reliability of the instrument. 

 

Sophisticated educational media, such as single-user and multi-user virtual environments, extend the nature of the performance challenges presented and the knowledge and cognitive processes assessed. Single-user and multi-user virtual environments provide students with an “Alice in Wonderland” experience: Students have a virtual representation of themselves in the world, called an “avatar”; one may think of it as a “digital puppet.” This avatar enters the “looking glass” (monitor screen) to access a 3-D virtual world. 

 

Our results from a series of research studies show that these virtual environments enable students to engage in authentic inquiry tasks (problem finding and experimental design) and also increase students’ engagement and self-efficacy (Nelson, Ketelhut, Clarke, Bowman, Dede, 2005; Clarke, Dede, Ketelhut, & Nelson, 2006; Ketelhut, 2007; Nelson, 2007; Clarke & Dede, 2007). 

 

We have also found that student performance on the multiple choice post-tests do not necessarily reflect learning that we see via interviews, observations, summative essays, and analyses of log file data that capture students’ activity as they interact with the environment (Clarke, 2006; Ketelhut, Dede, Clarke, Nelson, Bowman, 2008; Ketelhut & Dede, 2006). 

 

have built rich case studies of student learning in which we triangulate and compare different data sources, both qualitative and quantitative in order to illustrate and understand students’ inquiry learning (Clarke, 2006; Ketelhut et al, 2008; Clarke & Dede, 2005; Clarke & Dede, 2007). 

 

As a finding of our studies on how virtual environments foster inquiry learning, we have established that the multiple choice assessments we use, even after extensive refinement, do not fully capture students’ learning of inquiry skills. 

 

We found that MUVE environments and similar interactive, immersive media enable the collection of very rich observations about individual learners that provide insights on how students learn and engage in the inquiry processes (Ketelhut et al, 2008; Clarke, 2009b). 

 

Research on game-like simulations for fostering student learning is starting to proliferate, yet studying the potential of this medium for assessing student learning in a standardized setting is as yet untapped. We believe that virtual performance assessments developed with immersive technologies have the potential to provide better evidence for assessing inquiry processes. 

 

With funding from the Institute of Education Sciences (IES), we are developing and studying the feasibility of virtual performance assessments to assess scientific inquiry for use in school settings as a standardized component of an accountability program (see Clarke, 2009a; virtualassessment.org). The research questions we are addressing in this project are: RQ 1: Can we construct a virtual assessment that measures scientific inquiry, as defined by the NSES? What is the evidence that our assessments are designed to test NSES inquiry abilities? RQ 2: Are these assessments reliable? 

 

ECD is a comprehensive framework that contains four stages of design: domain analysis, domain modeling, conceptual assessment framework and compilation, and a four-phase delivery architecture. Phases 1 and 2 focus on the purposes of the assessment, nature of knowing, and structures for observing and organizing knowledge. Phase 3 is related to the Assessment Triangle; in this stage, assessment designers focus on the student model (what skills are being assessed), the evidence model (what behaviors/performances elicit the knowledge and skills being assessed), and the task model (what situations elicit the behaviors/evidence). Like the Assessment Triangle, these aspects of the design are interrelated. In the compilation stage of Phase 3, tasks are created. The purpose is to develop models for schema-based task authoring and developing protocols for fitting and estimation of psychometric models. Phase 4, the delivery architecture, focuses on the presentation and scoring of the task (Mislevy et. al. 2003, Mislevy & Haertel, 2006). 

 

Before students enter the environment, they watch a video that introduces them to the narrative. The first part of the assessment borrows from game design and sends students on quests that lead them to making observations and inferences about the kelp. They spend time gathering information and then are asked to identify the problem. These early stages of inquiry such as data gathering and problem identification are difficult to capture in a multiple choice test, but easily captured via a student’s movement and actions in the world. Students are to also talk to the NPCs in the world and make decisions about questions to ask them and what type of tests they need to conduct to gather data. These actions are all recorded and provide observations about students’ inquiry processes. 

 

However, we are not simply capturing or counting clicks. We are creating ways to triangulate performances by having students provide feedback on why they collected data, or why they made a particular choice. We are developing performance palettes as an interactive method for assessing student product and performance (see figures 6 and 7). We are utilizing current technologies that will allow us to rely less on text and more on interactive representations such as ‘drag and drop’ text interfaces and audio queries. 

 

In a student’s inquiry process, s/he moves from problem identification to hypothesizing and questions and ultimately, investigating. Through narrative, we can continually update students with accurate information they need to progress through the different phases of inquiry, such as problem identification and 

 

hypothesizing, thereby ensuring various phases of assessing their understanding are independent, not interdependent. 

 

Validity is a central issue in test construction. According to Messick, “validity is an integrated evaluative judgment of the degree to which empirical evidence and theoretical rationales support the adequacy and appropriateness of inferences and actions based on test scores or other modes of assessment. . .” (cited in Messick, 1994 p.1). 

 

In our work in developing virtual inquiry curricula, we developed the ability to simulate the passing of time, to allow students to collect data on change over time, and to conduct experiments where time can be fast-forwarded. 

 




%% Import Date: 2023-08-08T05:18:05.709-07:00 %%

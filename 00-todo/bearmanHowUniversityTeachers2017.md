---
DOI: 10.1007/s10734-016-0027-7
Date: 2017
Rating: 0/5
Title: How university teachers design assessments a cross-disciplinary study
annotation-target: bearmanHowUniversityTeachers2017.pdf
tags: 
---


#### [How university teachers design assessments: a cross-disciplinary study](bearmanHowUniversityTeachers2017.pdf)


> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Bearman, M., Dawson, P., Bennett, S., Hall, M., Molloy, E., Boud, D., & Joughin, G. (2017). How university teachers design assessments: A cross-disciplinary study. _Higher Education_, _74_(1), 49–64. [https://doi.org/10.1007/s10734-016-0027-7](https://doi.org/10.1007/s10734-016-0027-7)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 




#### Annotations


Assessment is critical to both student learning and student progression in higher education. It promotes learning as well as confirming attainment of standards. 

 

Assessment design and its associated processes is one of the most critical aspects of assessment practice (Carless 2015; Gibbs and Simpson 2004). Assessment design can be defined to include all processes that take place in order to form specific assessment tasks for a particular course or unit, including the selection and timing of tasks, development of rubrics and re-development of a task in response to student performance. It does not include the individual feedback provided to a student on a particular task, although we suggest that grading and feedback are strongly influenced by task design elements, such as rubrics or scheduling. 

 

While assessment design is often the key to promoting student learning (Carless 2015; Gibbs and Simpson 2004), it is troublesome for many. In a recent phenomenographic study of conceptions of assessment in higher education, Postareff et al. (2012) suggest that the “most striking finding was that the assessment practices in the study context were mostly conventional and that most academics had difficulty describing the purpose of assessment”. Price et al (2011) suggest that educators’ `assessment ‘literacy’ levels are low and that this has an impact upon assessment practice. Even if educators are familiar with current scholarly notions of good assessment practice they must then be able find ways of developing assessment tasks which take into account their circumstances and pedagogical beliefs and aspirations. 

 

Many studies report gaps between their educators’ aims for their assessments and how they are actually implemented in practice (QuesadaSerra et al. 2014; Norton et al. 2005; Norton et al. 2013; Fletcher et al. 2012). For example, Norton et al (2013) note that of 586 educators undertaking postgraduate qualifications across UK institutions, only 12% of survey respondents reported that their teaching philosophy and assessment practices were aligned, with 57% of respondents reporting that they were ‘uncertain’. Adding to the complexity, educators may believe their assessment philosophies and practices are in alignment, when in reality they may not be (Offerdahl and Tomanek 2011). In other words, the dual challenge is for educators to learn about assessment and to systematically integrate this understanding into their everyday work. 

 

Meyer et al (2010) propose relationships between: the assessment literature; professional regulatory bodies including program review processes; the institution and associated strategic plans and policies; teaching staff, including 

 

expectations, research, professional development and codes of conduct; and students. Macdonald and Joughin (2009) echo these, suggesting additional influences on assessment practice: the module and program design; innovation and improvement within a program; departmental culture; institutional resources; institutional recognition; and the external broader higher education context. These frameworks are essentially conceptual rather than empirical. They provide an overview of the various components and relationships in an assessment system. They do not provide insight into what educators actually do, however this knowledge may be key in changing ‘on the ground’ assessment practices. 

 

An in-depth qualitative exploration of assessment design may provide the necessary insights. To the best of our knowledge, there has not been this type of study focussed specifically on how educators’ design assessment, although previous work has identified a range of influences on educators’ assessment processes. 

 

These include: the tension between summative and formative purposes of assessment (Meyer et al. 2010); the constraints of administrative requirements (Meyer et al. 2010; Norton et al. 2005); the influence of the individual discipline on how assessment is practiced (Norton et al. 2005); and the ‘backwash’ effect – that is the impact on future design of the educator observing students engaging with the assessment task (Watkins et al. 2005). The diversity of these factors indicate the complexity of assessment design process, with the associated corollary that supporting educators’ assessment design practices is both necessary and requires a nuanced approach. A deep qualitative understanding of how educators design assessment may also assist in developing these supports. 

 

To summarise this diverse and brief exploration of the literature, for many individual educators, situated in a system of institutional regulations and relationships, there is a dissonance between what they would regard as ideal assessment design and what they actually do. Previous work indicates a complex terrain, which has not been fully explored. There is a need to understand how educators design assessments ‘on the ground’ in order to support them to achieve and expand their aspirations. 

 

This research was undertaken as a foundational study to underpin the development of resources for university educators. We believe that describing how academics enact assessment design within their local environments is critical to understanding and improving assessment in higher education. There are many practical implications; a deeper understanding of assessment practice allows educators to understand and manage their own circumstances as well as permitting institutions to optimise conditions which encourage innovative and exceptional assessment design. 

 

The educators themselves naturally were the most significant factor in how the assessments developed; these professional influences reflected the complex intersection of personal histories and professional identities. 

 

Educators’ past experiences of conducting assessment were strongly influential. Experiences of previous iterations of the assessment task – or similar assessment tasks possibly even at a different institution – were particularly important. 

 

Novices to assessment design were distinguished by focussing on past experiences of others’ teaching, particularly role models. 

 

Interviewees also drew from experiences of assessment which occurred to them as undergraduates, postgraduates or in the professional workplace. 

 

Beliefs about assessment were broad and varied, and often were co-constructed with other professional and environmental influences. Educators held beliefs about students and how they learnt; they held beliefs around types and modalities of assessment; and they described a range of conflicting beliefs about the purpose and form of grades. 

 

The professional identity of the participants influenced their assessment designs. Many did not identify as educators; on the other hand, some saw their role as primarily educational. Their particular discipline or vocation could strongly influenced assessment design and implementation. 

 

Educators’ professional identities were closely intertwined with the characteristics and circumstances of the educator, which were also linked to their beliefs about assessment and the past experiences with assessment. 

 

The theme influences from the environment was categorised into a range of subthemes, which represented the diverse and intertwined range of institutional, collegial and student-derived influences. All of these influences were necessarily mediated by the educators who were designing assessments, and so there was a strong interaction between the educators’ own internal drivers and the ones which were external to them. 

 

The most ubiquitous influences related to the unit or program. These included factors such as: unit documentation; how the unit was ‘handed over’ or inherited; size of the unit; mode of delivery; the unit’s place in the program; the goals of the unit or program; desired student learning outcomes; and the complex influences of teamwork and negotiation. 

 

Educators saw student learning as a strong influence on assessment design. This included formal learning outcomes and objectives but was also strongly linked to broad ideas about the student’s future trajectory outside the academy. 

 

Educators were also influenced by organisational requirements. These included handbook entries, mapping of assessment to standards or frameworks and a range of approval processes. In considering this subtheme, it was difficult to disentangle formal requirements, recommendations, 

 

norms and myths. The presence of institutional requirements generally promoted keeping the status quo rather than promoting change. 

 

The presence of institutional requirements generally promoted keeping the status quo rather than promoting change. 

 

As is clear from the previous quotes, there were also influences relating to organisational culture. This worked at different layers, from the immediate department to the broader institution. It encompassed division of work and teaching roles, the spoken but unwritten ways of doing things, and the implicit and unspoken ways of doing things. 

 

Responses and interactions which provided data or feedback regarding a specific assessment also formed a significant part of the assessment design process. Formal and informal peer review strongly influenced assessment design. 

 

There were resource influences, including time, money, technologies, scheduling and other logistical factors, people and suitable spaces. 

 

Against this backdrop of influences, educators undertook the activities which led to design, development and implementation of their assessment. These are categorised into three themes: essential, selective and ‘meta’ design activities. 

 

Educators developed assessment tasks, determining the form as well as the content of the assessment task itself. Educators designed marking/feedback processes, such as rubrics, feedback timing and quantity, moderating marks, using peer or other feedback innovations, and the use of technology to support processes. Aligning teaching and assessment activities was a large concern. This included structuring assessments and assessment schedules, so that they linked to other teaching and assessment tasks, and optimised feedback, resources and student workload. 

 

hese concerns were individual and diverse. A subset of these activities was primarily concerned with the grade-bearing aspect of the assessments. 

 

Some educators actively took actions to reduce cheating through peer assessment, continual changes to assessment, and providing students with skills to avoid plagiarism. Some educators tried to ensure student equity. 

 

Educators also adapted their assessments to manage degrees of difficulty. This was an activity which included consideration of what the standards should be, what the students’ capabilities were, increasing complexity of tasks and matching different tasks to different students. 

 

Some educators actively promoted student engagement, with the assessment tasks. This included ‘intrinisic’ rewards such as making authentic and relevant tasks, peer assessment, rotating tasks, interactive activities such as role-play or using competition; and also ‘extrinsic rewards’ such as bonus marks, grades, compulsory assessments. 

 

Some educators actively promoted a higher degree of student agency. This included allowing students to determine topics or tasks, or taking a research orientation, but it sometimes caused tensions with grading and comparing non-equivalent tasks. 

 

The first of these activities was adapting against the constraints; when faced with a conflict between their personal and environmental influences, some educators designed an assessment that could accommodate both solutions. A similar, but even more deliberate activity, took place when educators strategized regarding how to achieve change. Approaches included negotiating with colleagues and managing collegial input, keeping the head of department informed, mapping the assessment against frameworks and criteria, ‘selling’ the assessment to students, deliberately managing the paperwork to support change, ensuring the right teaching team was available, and changing assessment over several iterations of a unit rather than all at once. 

 

While many educators experienced dissonances between what they wanted to achieve with assessment and what they actually implemented, many others did not: they found creative solutions when balancing competing demands. The following discussion both situates this study within the broader literature and presents key implications for approaching improving assessment processes and practices. 

 

On the one hand, the findings support the importance of the individual educator, their beliefs and the choices that they make. On the other hand, the analysis also recognises that the educator shapes, and is shaped by, a broader sociocultural environment. 

 

When the influences category is considered as a whole, the mutually constructed and interlinked nature of the personal and contextual factors is striking. The previously experienced environmental influences became subsumed into the personal history of the educator as they developed assessment during their academic careers. In other words, today’s assessment design experience becomes tomorrow’s assessment design influence. This suggests that any approach to improving assessment design should not ignore the past contexts and experiences. In the same way we ask our students to draw from previous experiences to form current conceptions, it may be worth explicitly asking educators to compare and contrast their current trajectories with their past experiences of assessments as learners and as teachers. 

 

Carless’ (2015) learning-oriented assessment model proposes relationships between learningoriented assessment tasks with a disciplinary focus, developing student evaluative judgement and student engagement with feedback. His model emphasises how learning oriented assessment is related to students developing ‘evaluative judgement’ – the capacity to identify and make complex judgements around markers of quality. 

 

Overall, the theme professional influences, indicates the breadth of disciplinary variation, with interviewees describing exposure to a wide array of assessment designs through different courses, institutions, countries and colleagues. We regard this as positive, as the breadth and diversity of approaches is appropriate for a complex teaching process like assessment. However, it may also mean that educators may lack the capacity to see beyond their particular experience. We note that, with some exceptions, the language of assessment literature was absent. Educators often spoke passionately and thoughtfully about their assessment designs enhancing student learning in a very broad sense but without reference to literature or theory. 

 

James (2014) suggests that explicit consideration of ‘doxa’ – that which is so accepted it is not noticed (Bourdieu 1977) – is relevant to assessment practice and this study bears this out. Some participants focussed on disciplinary traditions such as exams or essays simply because they could not envisage an alternative. Likewise, many participants overlooked the role of formative assessment and focussed mostly on gradebearing tasks. This issue, of expansion beyond the familiar, may be an area where organisational supports may be required to change assessment practice. 

 

The role of the departmental leadership was particularly notable and appeared at its most powerful when the departments were small. The head of department seemed to determine the significance and merit of assessment innovation in several ways: firstly, through control of resources; secondly, through the ‘status’ afforded teaching in general and assessment in particular; and finally through their own beliefs as to what constituted good assessment. This role of local leadership in promoting assessment practice also underlines the social dimension of innovation. 

 

Educators emphasised the value of colleagues’ experiences, corridor conversations and workshops but rarely mentioned reading the higher education literature. This suggests that improving assessment through professional development may not just be about individuals, but about relationships. This raises some key questions: how can we join individual learning to organisational learning? How can professional development in assessment be seen as more like a social privilege (such as a sabbatical) not an individual burden (such as mandatory training)? Do departments, faculties and institutions need to think about time within local face-toface contexts to share, discuss and strategise about assessment? 

 

It may be that if we value assessment development in the way we value research development with its focus on teams, collegiate exchange and the value of strategic thinking, we may see some striking results. 

 




#### Related
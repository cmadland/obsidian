---
DOI: 10/fpnss4
Date: 1998
Rating: 0/5
Title: "Assessment and Classroom Learning"
ShortSummary: ""
annotation-target: blackAssessmentClassroomLearning1998.pdf
---


#### [Assessment and Classroom Learning](blackAssessmentClassroomLearning1998.pdf)




> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Black, P., & Wiliam, D. (1998). Assessment and Classroom Learning. _Assessment in Education: Principles, Policy & Practice_, _5_(1), 7–74. [https://doi.org/10/fpnss4](https://doi.org/10/fpnss4)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 


#### Related

#### Annotations


This article is a review of the literature on classroom formative assessment. Several studies show firm evidence that innovations designed to strengthen the frequent feedback that students receive about their learning yield substantial learning gains. The perceptions of students and their role in self-assessment are considered alongside analysis of the strategies used by teachers and the formative strategies incorporated in such systemic approaches as mastery learning. There follows a more detailed and theoretical analysis of the nature of feedback, which provides a basis for a discussion of the development of theoretical models for formative assessment and of the prospects for the improvement of practice. 

 

Our primary focus is the evidence about formative assessment by teachers in their school or college classrooms. 

 

The principal reason for this is that the term formative assessment does not have a tightly defined and widely accepted meaning. 

 

In this review, it is to be interpreted as encompassing all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the 

 

teaching and learning activities in which they are engaged. 

 

Two substantial review articles, one by Natriello (1987) and the other by Crooks (1988) in this same field serve as baselines for this review. 

 

A second approach was to search by key-words in the ERIC data-base; this was an inefficient approach because of a lack of terms used in a uniform way which define our field of interest. 

 

Natriello's review covered a broader field than our own. The paper spanned a full range of assessment purposes, which he categorised as certification, selection, direction and motivation. 

 

Only the last two of these are covered here. Crooks used the term 'classroom evaluation' with the same meaning as we propose for 'formative assessment'. These two articles gave reference lists containing 91 and 241 items respectively, but only 9 items appear in both lists. This illustrates the twin and related difficulties of defining the field and of searching the literature. 

 

Natriello reviews the issues within a framework provided by a model of the assessment cycle, which starts from purposes, then moves to the setting of tasks, criteria and standards, then through to appraising performance and providing feedback and outcomes. 

 

Crooks' paper has a narrower focus—the impact of evaluation practices on students—and divides the field into three main areas—the impact of normal classroom testing practices, the impact of a range of other instructional practices which bear on evaluation, and finally the motivational aspects which relate to classroom evaluation. 

 

He concludes that the summative function of evaluation—grading—has been too dominant and that more emphasis should be given to the potential of classroom assessments to assist learning. 

 

However, in Crooks' view the 'most vital of all the messages emerging from this review' (p. 470) is that the assessments must emphasise the skills, knowledge and attitudes perceived to be most important, however difficult the technical problems that this may cause. 

 

In consequence, decisions about what to include have been somewhat arbitrary, so that we now have some sympathetic understanding of the lack of overlap between the literature sources used in the two earlier reviews. 

 

. Examples in Evidence Classroom Experience 

 

25 Portuguese teachers of mathematics were trained in self-assessment methods on a 20-week part-time course, methods which they put into practice as the course progressed with 246 students of ages 8 and 9 and with 108 older students with ages between 10 and 14 (Fontana & Fernandes, 1994). 

 

Both groups showed significant gains over the period, but the experimental group's mean gain was about twice that of the control group's for the 8 and 9-year-old students—a clearly significant difference. 

 

However, the work also illustrates that an initiative can involve far more than simply adding some assessment exercises to existing teaching—in this case the two outstanding elements are the focus on self-assessment and the implementation of this assessment in the context of a constructivist classroom. On the one hand it could be said that one or other of these features, or the combination of the two, is responsible for the gains, on the other it could be argued that it is not possible to introduce formative assessment without some radical change in classroom pedagogy because, of its nature, it is an essential component of the pedagogic process. 

 

The second example is reported by Whiting et al. (1995), the first author being the teacher and the co-authors university and school district staff. The account is a review of the teacher's experience and records, with about 7000 students over a 

 

period equivalent to 18 years, of using mastery learning with his classes. 

 

Like the previous study, this work has ecological validity—it is a report of work in real classrooms about what has become the normal method used by a teacher over many years. 

 

The result shows two characteristic and related features—the first being that the teaching change involves a completely new learning regime for the students, not just the addition of a few tests, the second being that precisely because of this, it is not easy to say to what extent the effectiveness depends specifically upon the quality and communication of the assessment feedback. 

 

The project was an experiment in mathematics teaching (Martinez & Martinez, 1992), in which 120 American college students in an introductory algebra course were placed in one of four groups in a 2 X 2 experimental design for an 18-week course covering seven chapters of a text. Two groups were given one test per chapter, the other two were given three tests per chapter. 

 

Two groups were taught by a very experienced and highly rated teacher, the other two by a relatively inexperienced teacher with average ratings. 

 

The results of a post-test showed a significant advantage for those tested more frequently, but the gain was far smaller for the experienced teacher than for the newcomer. 

 

In example number five (Butler, 1988), the work was grounded more narrowly in an explicit psychological theory, in this case about a link between intrinsic motivation and the type of evaluation that students have been taught to expect. 

 

For feedback, one-third of the group were given individually composed comments on the match, or not, of their work with the criteria which had been explained to all beforehand. A second group were given only grades, derived from the scores on the preceding session's work. The third group were given both grades and comments. 

 

For the 'comments only' group the scores increased by about one-third between the first and second sessions, for both types of task, and remained at this higher level for the third session. The 'comments with grade' group showed a significant decline in scores across the three sessions, particularly on the convergent task, whilst the 'grade only' group declined on both tasks between the first and last sessions, but showed a gain on the second session, in the convergent task, which was not subsequently maintained. 

 

A significant feature here is that even if feedback comments are operationally helpful for a student's work, their effect can be undermined by the negative motivational effects of the normative feedback, i.e. by giving grades. 

 

The results are consistent with literature which indicates that task-involving evaluation is more effective than ego-involving evaluation, to the extent that even the giving of praise can have a negative effect with low-achievers. 

 

They also support the view that pre-occupation with grade attainment can lower the quality of task performance, particularly on divergent tasks. 

 

The first is that, whilst the experiment lacks ecological validity because it was not part of or related to normal curriculum work and was not carried out by the students' usual teachers, it nevertheless might illustrate some important lessons about ways in which formative evaluation feedback might be made more or less effective in normal classroom work. The second lesson is the possibility that, in normal classroom work, the effectiveness of formative feedback will depend upon several detailed features of its quality, and not on its mere existence or absence. A third message is that close attention needs to be given to the differential effects between low and high achievers, of any type of feedback. 

 

n this work (Schunk, 1996), 44 students in one USA elementary school, all 9 or 10 years of age, worked over seven days on seven packages of instructional materials on fractions under the 

 

instructions of graduate students. 

 

Students worked in four separate groups subject to different treatments—for two groups the instructors stressed learning goals (learn how to solve problems) whilst for the other two they stressed performance goals (merely solve them). 

 

For each set of goals, one group had to evaluate their problem-solving capabilities at the end of each of the first sessions, whereas the other was asked instead to complete an attitude questionnaire about the work. 

 

Outcome measures of skill, motivation and self-efficacy showed that the group given performance goals without self-evaluation came out lower than the other three on all measures. The interpretation of this result suggested that the effect of the frequent self-evaluation had out-weighed the differential effect of the two types of goal. 

 

There were two groups who differed only in the types of goal that were emphasised—the aim being to allow the goal effects to show without the possible overwhelming effect of the frequent self-evaluation. As expected, the learning goal orientation led to higher motivation and achievement outcomes than did the performance goal. 

 

The seventh example involved work to develop an inquiry-based middle school science-based curriculum (Frederiksen & White, 1997). The teaching course was focused on a practical inquiry approach to learning about force and motion, and the work involved 12 classes of 30 students each in two schools. 

 

Each class was divided into two halves: a control group used some periods of time for a general discussion of the module, whilst an experimental group spent the same time on discussion, structured to promote reflective assessment, with both peer assessment of presentations to the class and self-assessment. 

 

On the mean project scores, the experimental group showed a significant overall gain; however, when the students were divided into three groups according to low, medium or high scores on the initial basic skills test, the low scoring group showed 

 

a superiority, over their control group peers, of more than three standard deviations, the medium group just over two, and the high group just over one. 

 

Amongst the students in the experimental group, those who showed the best understanding of the assessment process achieved the highest scores. 

 

The eighth and final example is different from the others, in that it was a meta-analysis of 21 different studies, of children ranging from pre-school to grade 12, which between them yielded 96 different effect sizes (Fuchs & Fuchs, 1986). 

 

The second feature is that the main learning gains from the formative work were only achieved when teachers were constrained to use the data in systematic ways which were new to them. 

 

It is hard to see how any innovation in formative assessment can be treated as a marginal change in classroom work. 

 

All such work involves some degree of feedback between those taught and the teacher, and this is entailed in the quality of their interactions which is at the heart of pedagogy. 

 

The nature of these interactions between teachers and students, and of students with one another, will be key determinants for the outcomes of any changes, but it is difficult to obtain data about this quality from many of the published reports. 

 

Another evaluation problem that arises here is that almost all innovations are bound to be pursuing innovations in ends as well as in means, so that the demand for unambiguous quantitative comparisons of effectiveness can never be fully satisfied. 

 

For assessment to be formative the feedback information has to be used—which means that a significant aspect of any approach will be the differential treatments which are incorporated in response to the feedback. 

 

The role of students in assessment is an important aspect, hidden because it is taken for granted in some reports, but explicit in others, particularly where self and peer assessments by and between students are an important feature (with some arguing that it is an inescapable feature—see Sadler, 1989). 

 

However, the consistent feature across the variety of these examples is that they all show that attention to formative assessment can lead to significant learning gains. Although there is no guarantee that it will do so irrespective of the context and the particular approach adopted, we have not come across any report of negative effects following on an enhancement of formative practice. 

 

One example, the kindergarten study of Bergan et al. (1991) brings out dramatically the importance that may be attached to the achievement of such gains. This particular innovation has changed the life chances of many children. 

 

Assessment by Teachers Current Practice 

 

Teachers' practices in formative assessment were reviewed in the articles by Crooks (1988) and Black (1993b). Several common features emerged from these surveys. The overall picture was one of weak practice. Key weaknesses were: 

 

Classroom evaluation practices generally encourage superficial and rote learning, concentrating on recall of isolated details, usually items of knowledge which pupils soon forget. 

 

Teachers do not generally review the assessment questions that they use and do not discuss them critically with peers, so there is little reflection on what is being assessed. 

 

The grading function is over-emphasised and the learning function underemphasised. 

 

There is a tendency to use a normative rather than a criterion approach, which emphasises competition between pupils rather than personal improvement of each. The evidence is that with such practices the effect of feedback is to teach the weaker pupils that they lack ability, so that they are de-motivated and lose confidence in their own capacity to learn. 

 

Teachers appear to be unaware of the assessment work of colleagues and do not trust or use their assessment results (Cizek et al, 1995; Hall et al, 1997). 

 

There is little focus on such outcomes as speculation and critical reflection (Stiggins et al, 1989; Schilling et al., 1990; Pijl, 1992; Bol & Strage, 1996; Senk et al, 1997), and students focus on getting through the tasks and resist attempts to engage in risky cognitive activities (Duschl & Gitomer, 1997). 

 

Although teachers can predict the performance of their pupils on external tests—albeit tests reflecting low-level aims— their own assessments do not tell them what they need to know about their students' learning (Lorsbach et al, 1992; Rudman, 1987). 

 

a survey of teachers in Quebec Province, Canada, reports that for formative assessment 'Indeed they pay lip service to it but consider that its practice is unrealistic in the present educational context' (quoted by Dassa et al., 1993, p. 116). 

 

The conclusion of a survey about practice in Belgian primary schools was that the criteria used by teachers were 'virtually invalid by external standards' (Grisay, 1991, p. 104). 

 

A study which used interviews and so produced a richer picture of the perceptions of US teachers concludes as follows: Most of the teachers in this study were caught in conflicts among belief systems, and institutional structures, agendas, and values. The point of friction among these conflicts was assessment, which was associated with very powerful feelings of being overwhelmed, and of insecurity, guilt, frustration, and anger. These teachers expressed difficulty in keeping track of and having the language to talk about children's literate development. They also described pressure from external accountability testing. They differed in their assessment strategies and in the language they used to describe students' literacy development. Those who worked in highly controlling situations were inclined to use blaming language and tended to 

 

provide global, negative descriptive assessments in impersonal language. Their assessments were likely to be based on a simple, linear notion of literacy. The less controlling the situation the less this was likely to occur. This study suggests that assessment, as it occurs in schools, is far from a merely technical problem. Rather, it is deeply social and personal. Johnston et al., 1995, p. 359) 

 

The effects here run deep, witness the evidence in Britain that when teachers were required to undertake their own assessments they imitated the external tests ), and seemed to be able to think only in terms of frequent summative tests with no feedback action (Ratcliffe, 1992; Harlen & Malcolm, 1996). 

 

Assessment, Pedagogy and Innovation 

 

Given these problems, it is not surprising that when national or local assessment policies are changed, teachers become confused. 

 

Where changes have been introduced with substantial training or as an intrinsic part of a project in which teachers have been closely involved, the pace of change is slow because it is very difficult for teachers to change practices which are closely embedded within their whole pattern of pedagogy (Torrie, 1989; Shepard et al., 1994, 1996; Shepard, 1995) and many lack the interpretive frameworks that they need to co-ordinate the many separate bits of assessment information in the light of broad learning purposes (Bachor & Anderson, 1994). 

 

The aim has been to highlight some key points which are relevant to the main purpose of this review. The three outstanding features are: • that formative assessment is not well understood by teachers and is weak in practice; • that the context of national or local requirements for certification and accountability will exert a powerful influence on its practice; and • that its implementation calls for rather deep changes both in teachers' perceptions of their own role in relation to their students and in their classroom practice. 

 

These features have implications for research into this area. Research which simply interrogates existing practice can probably do little more than confirm the rather discouraging findings reported above. To be productive therefore, research has to be linked with a programme of intervention. If such intervention is to seek implementation with and through teachers in their normal classrooms, it will be changing their roles and ways of teaching; then the formative initiative will be part of a larger pattern of changes and its evaluation must be seen in that larger context. 

 

Students and Formative Assessment 

 

The core of the activity of formative assessment lies in the sequence of two actions. The first is the perception by the learner of a gap between a desired goal and his or her present state (of knowledge, and/or understanding, and/or skill). The second is the action taken by the learner to close that gap in order to attain the desired goal (Ramaprasad, 1983; Sadler, 1989). 

 

For the purposes of this review, the involvement of students in formative assessment will be considered by division into two broad topics, as follows: 

 

The first of these will focus on those factors which influence the reception of the message and the personal decisions about how to respond to it. 

 

The second will focus on the different ways in which positive action may be taken and the regimes and working contexts in which that action may be carried out. 

 

Goal Orientation 

 

The importance of these features arises from the conjunction of two types of research results summarised above. One is that the 'personal features' referred to above can have important effects on a student's learning. The other is that the way in which formative information is conveyed to a student, and the context of classroom culture and beliefs about ability and effort within which feedback is interpreted by the individual recipient, can affect these personal features for good or ill. 

 

Assessment by Students 

 

The first is to aim to develop the capacity of the student to recognise and appraise any gaps and leave to the student the responsibility for planning and carrying out any remedial action that may be needed. This first option implies the development within students of the capacity to assess themselves, and perhaps to collaborate in assessing one another. 

 

Arthur (1995) reported that the requisite skills are not purposefully taught in most programmes, but also described new research to develop these skills in nursing education. The motive given here is that the future professional will need all of the skills necessary for life-long learning, and self-evaluation must be one of these. The Norwegian initiative started from a more fundamental motive, which was to see selfand peer-assessment as an intrinsic part of any programme which aims to help students to take more responsibility for their own learning. 

 

A different slant on this aspect is provided in the study by James of recorded dialogues between teachers and students (1990). This study showed that in such dialogues, the teacher's power easily overwhelms the student's contribution, the latter being too modestly tentative. 

 

The assumption here is they cannot do so unless they can first understand the goals which they are failing to attain, develop at the same time an overview in which they can locate their own position in relation to those goals, and then proceed to pursue and internalise learning which changes their understanding (Sadler, 1989). In this view, self-assessment is a sine qua non for effective learning. 

 

Studies of Self-assessment 

 

Self-evaluation is an intrinsic aspect of reflection on one's own learning. 

 

Baird et al. (1991) reported on work with 27 teachers and 350 students where teachers were helped to know more about their students and to learn more about how they might change the style of classroom work by a strategy based on meta-cognition and constructivism. 

 

A larger scale innovation is fully described in a book by Ross et al. (1993). The aim was to change assessment of achievement in the visual arts by bringing students into the assessment process as reflective practitioners, mainly through the development of 'assessment conversations' in which students were encouraged to reflect on their work and to articulate their reflections. 

 

The authors are enthusiastic in their accounts of the success of their work, and believe that the students involved showed that they 'are capable of rich and sophisticated responses to and understandings of their own work ... in collaboration with their conversation partner' (p. 161). They concluded that the approach opened up new opportunities in aesthetic knowing and appraisal, but that it also required that teachers abandon traditional assessment practices. 

 

Similarly qualitative reports were given of an initiative to hand over all responsibility for assessment of a first-year undergraduate course to students' selfassessment (Edwards & Sutton, 1991), 

 

the initiative produced a significant change in students' commitment to their work and there was also some indirect evidence in both of improvement in their learning achievement. 

 

Peer-assessment 

 

The reliabilities of selfand peer-assessments were also investigated, in work with college biology students, by Stefani (1994). He found correlations with teachers' assessments of 0.71 for self-assessments and 0.89 for peer-assessments. All of the students said that the selfand peer-assessment work made them think more, and 85% said that it made them learn more. 

 

Links to Theories of Learning 

 

The arguments given by Zessoules & Gardner (1991) show how any assessment changes of the types described above might be expected to enhance learning if they help students to develop reflective habits of mind. They further argue that such 

 

development should be an essential component in programmes for the implementation of authentic assessment in classroom practice. 

 

In a review of European research in this field, Elshout-Mohr (1994) points out both that students are often unwilling to give up misunderstandings—they need to be convinced through discussion which promotes their own reflection on their thinking—and also that if a student cannot plan and carry out systematic remedial learning work for himself, he or she will not be able to make use of good formative feedback. Both of these indicate that self-assessment is essential. 

 

Strategies and Tactics for Teachers 

 

Choice of Task 

 

It is obvious that formative assessment which guides learners towards valued learning goals can only be generated with tasks that both work to those goals and that are open in their structure to the generation and display of relevant evidence, both from student to teacher and to students themselves. 

 

Dumas-Carre & Larcher (1987) were more ambitious. They emphasised the need to shift current pedagogy to give more emphasis to procedural aspects of knowledge and less to the declarative aspects. 

 

This scheme distinguished tasks which (a) presented a specific situation identical to the one studied, or (b) presented a 'typical' problem but not one identical to the one studied, requiring identification of the appropriate algorithm and its use, rather than exact replication of an earlier procedure as in (a), and (c) a quite new problem 

 

requiring new reasoning and construction of a new approach, deploying established knowledge in a new way. Students would need special and explicit training for tackling tasks of type (c). 

 

Discourse 

 

Newmann (1992) makes a plea, on similar grounds, for assessment in social studies to focus on discourse, defined by him as language produced by the student with the intention of giving narrative, argument, explanation or analysis. 

 

In the same vein, Quicke & Winter (1994) report success in work with low achieving students in Year 8, where they aimed to develop a social framework for dialogue about learning. The work of Ross et al. (1993) in aesthetic assessment in the arts can be seen as a response to this plea, and the difficulties reported by Radnor (1994) are evidence of the inadequacy of established practice. 

 

Radnor's paper uses the phrase 'qualitative formative assessment' in its title, and this may help to explain why quantitative evidence for the learning effects of discourse variations is hard to find. 

 

Questions 

 

The quality of classroom questioning is a matter for concern, as expressed in the work of Stiggins et al. (1989) who studied 36 teachers over a range of subjects and over grades 2 to 12, by observation of classroom work, study of their documentation, and interviews. At all levels the questioning was dominated by recall questions, and whilst those trained to teach higher-order thinking skills asked more relevant questions, their use of higher-order questions was still infrequent. 

 

Several authors report work focused on question generation by students, and as pointed out in the section on Self-assessment above, this may be seen as an extension of work on students' self-assessment. With college students, King (1990, 1992a,b; 1994) found that training which prompted students to generate specific thought-provoking questions and then attempt to answer them is more effective than training in other study techniques, which she interprets in terms of the strategy underlying the training which aimed to develop learner autonomy and learners' control over their own work. 

 

The Use of Tests 

 

Bangert-Drowns, et al. (1991b) reviewed the evidence of the effects of frequent class testing. Their meta-analysis of 40 relevant studies showed that performance improved with frequent testing and increased with increased frequency up to a certain level, but that beyond that (somewhere beyond 1 and 2 tests per week) it could decline again. 

 

The evidence also indicated that several short tests were more effective than fewer longer ones. 

 

When given a short formative quiz after each lecture students performed significantly better than they did when no quiz was provided on three measures— post-tests of familiar items, post-tests of unfamiliar items and a survey of satisfaction with the instruction. 

 

The care taken with this experiment shows how studies cannot be accepted as relevant without careful scrutiny of the experiment design, and of the quality of the questions used for both the treatment and the criterion tests. 

 

Behind these reservations lies the larger issue of whether or not the testing is serving the formative assessment function. This cannot be clarified without a study of how test results are interpreted by the students. If the tests are not used to give feedback about learning, and if they are no more than indicators of a final highstakes summative test, or if they are components of a continuous assessment scheme so that they all bear a high-stakes implication, then the situation can amount to no more than frequent summative testing. 

 

Tan (1992) describes a situation in a course for first year medical students in which he collected evidence that the frequent summative tests were having a profound negative influence on their learning. 

 

The Quality of Feedback 

 

Both of the previous sub-sections lead to the almost obvious point that the quality of the feedback provided is a key feature in any procedure for formative assessment. 

 

he instructional effect of feedback from tests was reviewed by Bangert-Drowns et al. (1991a) using a meta-analysis of 58 experiments taken from 40 reports. Effects of feedback were reduced if students had access to the answers before the feedback was conveyed. When this effect had been allowed for, it was then the quality of the feedback which was the largest influence on performance. Programmed instruction and simple completion assessment items were associated with the smallest effects. 

 

Feedback was most effective when it was designed to stimulate correction of errors through a thoughtful approach to them in relation to the original learning relevant to the task. 

 

The linkage of feedback to assumptions about the nature of the student learning which it is designed to encourage has been taken further in work on curriculumbased assessment by Fuchs et al. (1991). 

 

Formulation of Strategy The sub-sections above can be regarded as treatments of the various components of a kit of parts that can be assembled to compose a complete strategy. 

 

Weston et al. (1995) have argued that if the literature on formative assessment is to inform instructional design, then a common language is needed. 

 

For Ames (1992), the distinction between the performance and mastery perspectives is a starting point, but she then outlines three salient features, namely meaningful tasks, the promotion of the learners' independence by giving authority to their own decision making, and evaluation which focuses on individual improvement and mastery. 

 

The importance of changing the assumptions that teachers make about learning is recognised in this review. 

 

An account of a project to provoke and support teachers in making changes of this type (Torrie, 1989) brings 

 

out the many difficulties that teachers encountered, both in making their assessments relating to learning criteria, and in changing their teaching and feedback to break away from norm-referenced assumptions in supporting student's learning. 

 

Nichols's (1994) analysis goes deeper in concentrating on what he terms cognitively diagnostic assessments. It is pointed out that classical psychometrics has been directed towards the use of assessments to guide selection, and so a new relationship with cognitive science is needed if it is to be used to guide learning. 

 

Tests must be designed in the light of models of specific knowledge structures in order to help determine the progress of learners in acquiring those structures, so that the interpretation of the feedback can serve the purpose of making inferences about students' cognitive mechanisms. 

 

It is clear that many traditional types of test are inadequate to this purpose because they do not reveal the methods used by those tested. 

 

Lorsbach et al. (1992) explored the factors which affect the validity of assessment tasks when judged from a constructivist perspective and emphasised that a major threat to validity is the extent to which students can construct the meanings of the tasks intended by those who set them. 

 

However, the latter authors, developing the arguments in Torrance (1993), provide a more wide-ranging theoretical discussion, contrasting two approaches to formative assessment—a behaviourist one, stressing measurement against objectives, and a social constructivist one integrating the assessment into learning. 

 

Thus task selection, and the type of feedback that a task might generate, require a cognitive theory which can inform the link between learners' understanding and their interactions with assessment tasks, in the light of which assessment activities can be designed and interpreted. Such an approach will of course interact strongly with the pedagogy adopted, and may have to temper an a priori theoretical position with a readiness to adapt and develop by an inductive approach as formative feedback challenges the rationale of the work (Fuchs & Fuchs, 1986). The conclusion of the analysis is that a very substantial effort, involving collaboration between psychometricians, cognitive scientists and subject experts is needed. 

 

All of these discussions point to the need for very far-reaching changes if formative evaluation is to realise its potential. 

 

Systems General Strategies 

 

Good assessment feedback is either explicitly mentioned or strongly implied in 

 

eports of a range of studies and initiatives in which such feedback is one component of a broader strategy. 

 

Studies of Mastery Learning 

 

He proposed that success in learning was a function solely of the ratio of the time actually spent learning to the time needed for learning—in other words, any student could learn anything if they studied it long enough. 

 

Two main approaches to mastery learning were developed in the 1960s. One, developed by Benjamin Bloom, using teacher-paced group-based teaching approaches, was called Learning for Mastery (LFM) and the other was Keller's individual-based, student-paced Personalized System of Instruction (PSI). The vast majority of the research undertaken into mastery learning has been centred on Bloom's, rather than Keller's model, and that which has been done on PSI is largely confined to further and higher education. 

 

A key consequence of Bloom's LFM model is that students of differing aptitude will differ in their achievements unless those with less aptitude are given either a greater opportunity to learn or better quality teaching. 

 

The key elements in this strategy, according to McNeil (1969) were: 

 

The learner must understand the nature of the task to be learned and the procedure to be followed in learning it. 

 

The specific instructional objectives relating to the learning task must be formulated. 

 

It is useful to break a course or subject into small units of learning and to test at the end of each unit. 

 

The teacher should provide feedback about each learner's particular errors and difficulties after each test. 

 

The teacher must find ways to alter the time some students have available to learn. 

 

It may be profitable to provide alternative learning opportunities. 

 

Student effort is increased when small groups of two or three students meet regularly for as long as an hour to review their test results and to help one another overcome the difficulties identified by means of the test. 

 

 

[[Learning Pods!!]] 

Three major reviews of the research into the effectiveness of mastery learning use the technique of 'meta-analysis' to combine the results from a variety of different studies. The review of Block & Burns (1976) covers work done in the first half of the 1970s while Guskey & Gates (1986) and Kulik et al. (1990) cover the subsequent decade. 

 

Between them, the reviews by Block & Burns (1976) and Guskey & Gates (1986) provide 83 measures (from 35 studies) of the effect of mastery learning on general achievement, all using the 'Learning For Mastery' approach (LFM). They found an average effect size of 0.82, which is equivalent to raising the achievement of an 'average' student to that of the top 20%, and one of the largest average effects ever reported for a teaching strategy (Kulik & Kulik, 1989). 

 

The 1990 review by Kulik et al. looked at 108 studies which were judged to meet their criteria for inclusion in a meta-analysis. Of these, 91 were carried out with students over 18 years of age, 72 using Keller's PSI approach and 19 using Bloom's LFM approach. 

 

The effect sizes found were smaller than those found by Block & Bums and Guskey & Gates—not surprising given the greater representation of studies with older students. However, Kulik et al. also 

 

ound that the self-paced PSI approach tended to have smaller effect sizes than the teacher-paced LFM approach, and also seems to reduce completion rates in college courses. 

 

Others, most notably Robert Slavin, have questioned whether mastery learning is effective at all. In his own review of research on mastery learning, he criticises meta-analysis as being too crude, because of the way that results from all the research studies that satisfy the inclusion criteria are averaged. 

 

His own approach is to use a 'best-evidence' synthesis (Slavin, 1987), attaching more (necessarily subjective) weight to the studies that are well-designed and conducted. 

 

Although many of his findings agree with the meta-analytic reviews described above, he points out that almost all of the large effect sizes have been found on teacher-prepared, rather than standardised, tests, and indeed, the effect sizes for mastery learning measured by standardised tests are close to zero. 

 

This suggests that the effectiveness of mastery learning might depend on the 'curriculum-embeddedness' of the outcome measures. This is supported by Kulik et al.'s (1990) finding that the effect sizes for mastery learning as measured by formative tests (typically around 1.17) are greater than for summative tests (around 0.6). 

 

Slavin (1987) argues that this is because, in mastery learning studies where outcome is measured using teacher-produced tests, the teachers focus narrowly on the content that will be tested. In other words, the effects are produced (either consciously or unconsciously) by 'teaching to the test'. The crux of this disagreement is therefore the measure of 'mastery' of a domain—should it be the teacherproduced test or the standardised test? 

 

The Relevance of Mastery Learning 

 

The only clear messages emerging from the mastery learning literature are that mastery learning appears to be effective in raising students' scores on teacherproduced tests, is more effective in teacher-paced programmes than in self-paced programmes, and is more effective for younger students. 

 

There are at least five aspects of 'typical' mastery learning programmes that are relevant to the purposes of the present review: 

 

that students are given feedback; 

 

that students are given feedback on their current achievement against some expected level of achievement (i.e. the 'mastery' level); 

 

that such feedback is given rapidly; 

 

that such feedback is (or is at least intended to be) diagnostic; 

 

that students are given the opportunity to discuss with their peers how to remedy any weaknesses. 

 

Curriculum-based assessment (CBA) is a development which expanded in the late 1980s. The focus of many of the studies has been on early years education and the identification of pupils with special educational needs, but its methods and principles could apply right across the spectrum of education. 

 

(Shinn & Good III, 1993) sets out the central features of CBA as follows: 

 

Assessment exercises should faithfully reflect the main learning aims and should be designed to evoke evidence about learning needs. 

 

The main purpose for assessment is the formative purpose. 

 

Validity is paramount—seen as ensuring that instructional decisions taken on the basis of assessment evidence are justified. 

 

The focus of attention is the individual learner and individually attuned remedial action. 

 

The information from assessment should serve to locate the individual's attainment in relation to criteria for learning, but that this location should also be informed by norm data on the progress of others working to the same curriculum. 

 

The assessment should be frequent so that the trajectory of learning over time can be traced: the gradient of learning success is the key indicator—to follow each pupil's progress in general, and to indicate cases of special need. 

 

Portfolios 

 

The portfolio movement is more closely associated with efforts to change the impact of high-stakes, often standardised, testing of school learning. There is a vast literature associated with the portfolio movement in the USA. Much of it is reviewed, by Collins (1992), in the edited collections of Belanoff & Dickson (1991) and—for assessment of writing—by Calfee & Perfumo (1996a), whilst Courts & McInerney (1993), set out some of the issues in higher education. 

 

Mills (1996) gives an account of the origins of the innovation, describing the work as an attempt in Vermont to satisfy demands of accountability whilst avoiding the pressures of standardised tests. 

 

A portfolio is a collection of a student's work, usually constructed by selection from a larger corpus and often presented with a reflective piece written by the student to justify the selection. 

 

The involvement of the student in reviewing and selecting is seen as central— 

 

Calfee & Freedman (1996) see portfolios as offering a technology for helping the slogan of 'student-centred learning' to become a reality. 

 

However, there is little by way of research evidence, that goes beyond the reports of teachers, to establish the learning advantages. 

 

Attention has focused rather on the reliability of teachers' scoring of portfolios because of the motive to make them satisfy concerns for accountability, and so to serve summative purposes as well as the formative. 

 

Calfee & Perfumo (1996b) report on research with teachers into their experience of using portfolios. The results showed a disturbing gap between the general rhetoric and actual practice, for they showed that many teachers were paying little attention to external standards and were producing little evidence of any engagement of students in understanding why they are doing this work. 

 

Slater et al. (1997) describe an experiment in an introductory algebra course for college students which produced no significant difference in achievement between a group engaged in portfolio production and a control group. However, the achievement test was a 24-item multiple choice test, which might not have reflected some of the advantages of the portfolio approach, and at the same time the teacher reported that the portfolio group ended up asking more questions about real world applications and had been led to discuss more complex and interesting phenomena than the control group. I 

 

Summative Examination Models 

 

The Graded Assessment schemes in England were comprehensive provisions designed to replace terminal examinations for public certificates by a series of graded assessments, conducted in schools but moderated (i.e. checked for consistency of standards between schools) by an examining authority. 

 

In all of these accounts, one of the problems that stands out is the difficulties that teachers and developers met in trying to establish a criterion-referenced approach to assessment. 

 

In Canada, Dassa et al. (1993) describe the setting up of a bank of diagnostic items organised in a three-dimensional scheme: diagnostic context, notional content and cognitive ability, the items being derived from 

 

a study of common errors so that they could provide a basis for causal diagnosis. 

 

The problems of developing criterion-referenced assessment beset the far more radical reforms in Queensland (Withers, 1987; Butler, 1995). This Australian state abolished external examinations for secondary schools in 1971, but subsequently encountered problems in the quality and the norm-referencing of school-based assessments. 

 

Feedback 

 

The two concepts of formative assessment and of feedback overlap strongly. 

 

The Nature of Feedback 

 

In applying this model to the behavioural sciences, we can identify four elements making up the feedback system: 

 

data on the actual level of some measurable attribute; 

 

data on the reference level of that attribute; 

 

a mechanism for comparing the two levels, and generating information about the gap between the two levels; 

 

a mechanism by which the information can be used to alter the gap. 

 

In contrast, Ramaprasad (1983) defines feedback as follows: 

 

Feedback is information about the gap between the actual level and the reference level of a system parameter which is used to alter the gap in some way (p. 4). 

 

and specifically requires that for feedback to exist, the information about the gap must be used to alter the gap. If the information is not actually used in altering the gap, then there is no feedback. 

 

One of the most important reviews of the effectiveness of feedback was carried out by Kluger & DeNisi (1996). 

 

They found an average effect size of 0.4 (equivalent to raising the achievement of the average student to the 65th percentile), but the standard deviation of the effect sizes was almost 1, and around two in every five effects were negative. The fact that so many research reports found that feedback can have negative effects on performance suggests that these are not merely artefacts of poor design, or unreliability in the measures, but real, substantive effects. 

 

They began by noting that presented with a 'gap' between actual and reference levels of some attribute (what Kluger & DeNisi, 1996, term a 'feedback-standard discrepancy'), there are four broad classes of action. The first is to attempt to reach the standard or reference level, which is the typical response when the goal is clear, where the individual has a high commitment to achieving the goal and where the individual's belief in eventual success is high. 

 

The second type of response is to abandon the standard completely, which is particularly 

 

common where the individual's belief in eventual success is low (leading to 'learned helplessness'—Dweck, 1986). A third, and less extreme, response is to change the standard, rather than abandoning it altogether. Individuals may lower the standard, especially likely where they cannot or do not want to abandon it, and conversely, may, if successful, choose to raise the standard. The fourth response to a feedback-standard gap is simply to deny it exists. 

 

They identified three levels of linked processes involved in the regulation of task performance: meta-task processes:, involving the self; task-motivation processes, involving the focal task; and task learning processes involving the details of the focal task. 

 

Meta-task Processes 

 

feedback interventions that cue individuals to direct attention to the self rather than the task appear to be likely to have negative effects on performance. Thus praise, like other cues which draw attention to self-esteem and away from the task, generally has a negative effect (and goes some way to explaining why several studies, such as Good & Grouws (1975), found that the most effective teachers actually praise less than average). 

 

Further evidence of the negative effect of cueing pupils to focus on the self rather than the task comes from a study carried out by Butler (1987) in which she examined the effects of four kinds of feedback (comments, grades, praise, no feedback) on the performance of 200 Israeli grade 5 and 6 students in divergent thinking tasks. 

 

Within this framework, it can be seen that both internal and external motivation can be effective, but only when associated with internally, as opposed to externally, valued aims. 

 

The clear message from the research on attribution theory (see for example Vispoel & Austin, 1995) is that teachers must aim to inculcate in their students the idea that success is due to internal, unstable, specific factors such as effort, rather than on stable general factors such as ability (internal) or whether one is positively regarded by the teacher (external). 

 

Task Motivation Processes 

 

In contrast to those interventions that cue attention to meta-task processes, feedback interventions that direct attention towards the task itself are generally much more successful. 

 

Another important finding in Dempster's work is that tests promote learning as well as sampling it, thus contradicting the often quoted analogy that 'weighing the pig does not fatten it'. 

 

Task Learning Processes 

 

Feedback appears to be less successful in 'heavily-cued' situations such as are found 

 

in computer-based instruction and programmed learning sequences, and relatively more successful in situations requiring 'higher-order' thinking such as unstructured tests and comprehension exercises (Bangert-Drowns et al., 1991b) or concept mapping (Bernard & Naidu, 1992). 

 

Simmons & Cope (1993). In this study, pairs of children, aged 9-11, with little or no experience of Logo programming, showed higher levels of response (as measured by the SOLO taxonomy) when working on angle and rotation problems on paper than when working in a Logo environment, which the authors attributed to the propensity of the immediate feedback given in the Logo environment to encourage incremental or 'trial and improvement' strategies. 

 

In all this, it is easy to gain the impression that formative assessment is a static process of measuring the amount of knowledge currently possessed by the individual, and feeding this back to the individual in some way. However, as the meta-analysis of Fuchs & Fuchs (1986) showed, the effectiveness depends strongly on the systematic analysis and use of feedback by teachers. Furthermore, the account by Lidz (1995) of the history and literature of dynamic assessment (and particularly the work of Vygotsky and Feuerstein) makes plain that formative assessment is as much concerned with prediction (i.e. what someone can learn) as with what they have already learnt, and it is only in interaction with the learner (and the learning) that useful assessments can be made. 

 

Prospects for the Theory and Practice of Formative Assessment 

 

No Meta-analysis 

 

An underlying problem, which we have already noted in an earlier paper (Wiliam & Black, 1996), is that the term 'formative assessment' is not common in the assessment literature. Such meaning as we have attached to the term here is also represented for others by such terms as 'classroom evaluation', 'curriculum-based assessment', 'feedback', 'formative evaluation' and so on. 

 

Shinn & Good III (1993) argue that there needs to be a 'paradigm shift' in assessment, from what they call the current assessment paradigm (and what we have here called summative functions of assessment) to what they call the 'problem-solving paradigm' (broadly equivalent to what we are here calling the formative functions of assessment). 

 

Summative functions of assessment are concerned with consistency of decisions across (relatively) large groups of students, so that the over-riding imperative is that meanings are shared by different users of assessment results. A particular problem for the constructors of summative assessments is that exactly who will be making use of the assessment results is likely to be undetermined. In contrast, formative functions of assessment prioritise desirable consequences either for (relatively) small groups of students (such as a teaching group) or for particular individuals. 

 

Sadler built upon Ramaprasad's notion of the gap between the state revealed by feedback and the desired state, emphasising that action will be inhibited if this gap is seen as impracticably wide. He further argued that ultimately, the action to close that gap must be taken by the student—a student who automatically follows the diagnostic prescription of a teacher without understanding of its purpose or orientation will not learn. Thus self-assessment by the student is not an interesting 

 

option or luxury; it has to be seen as essential. 

 

Tittle's (1994) framework emphasises three dimensions. The first, the epistemology and theories involved, can relate both to positions held in relation to learning in general, and to the particular epistemology relevant to the subject matter concerned. 

 

The second dimension is the more evident one of the assessment characteristics; it can be remarked here that in several of the studies reported here, little is said about the detail of these, or about the distinctive effects of the particular subject matter involved. 

 

Tittle's third dimension brings in the interpreter and user, and she particularly stresses the importance of these. 

 

Thus current conceptions of validity provide no guide as to what 'ought' to be going on, merely a theoretical framework for discussing what is going on. 

 

These last two analyses bring out a feature which in our view has been absent from a great deal of the research we have reviewed. This is that all the assessment processes are, at heart, social processes, taking place in social settings, conducted by, on and for social actors. 

 

A study of seven experienced elementary school teachers examined the implicit criteria that teachers used to determine whether students had 'understood' something (Reynolds et ah, 1995). 

 

although they were regarded not as a static check-list, but rather as a series of potential clues to the level of the student's understanding: (1) changes in demeanour: students who had understood were 'bright-eyed' while those who had not appeared half-hearted; (2) extension of a concept: students who have understood something often take the idea further on their own initiative; (3) making modifications to a pattern: students who understand, spontaneously start making their own modifications, while those who don't understand imitate or follow rules; (4) using processes in a different context: students who have understood a particular idea often start seeing the same patterns elsewhere; (5) using shortcuts: only students who are sure of the 'big picture' can short-cut a procedure so that thinking up or using a short-cut is taken as evidence of understanding; (6) ability to explain: students who have understood something are usually able to explain it; (7) ability to focus attention: persistence on a task is taken as a sign of understanding. 

 

They also found that teachers used assessment results as if they gave information on what students knew, whereas, in fact, they were better indicators of motivation and task completion. 

 

Research—-prospects and needs 

 

A full list of important and relevant aspects would include the following: • the assumptions about learning underlying the curriculum and pedagogy; • the rationale underlying the composition and presentation of the learning work; 

 

• the precise nature of the various types of assessment evidence revealed by the learner's responses; • the interpretative framework used by both teachers and learners in responding to this evidence; • the learning work used in acting on the interpretations so derived; • the divisions of responsibility between learners and teachers in these processes; • the perceptions and beliefs held by the learners about themselves as learners about their own learning work, and about the aims and methods for their studies; • the perceptions and beliefs of teachers about learning, about the 'abilities' and prospects of their students, and about their roles as assessors; • the nature of the social setting in the classroom, as created by the learning and teaching members and by the constraints of the wider school system as they perceive and evaluate them; • issues relating to race, class and gender, which appear to have received little attention in research studies of formative assessment; • the extent to which the context of any study is artificial and the possible effects of this feature on the generalisability of the results. 

 

Particular attention ought to be paid to two specific problems. The first is the evidence in many studies that new emphasis on formative assessment is of particular benefit to the disadvantaged and low-attaining learners—evidence which is not supported in the results of other studies. 

 

The second problem, or clutch of problems, relates to the possible confusions and tensions, both for teachers and learners, between the formative and summative purposes which their work might have to serve. 

 

TABLE I. Different questions arising from Paradigm Shift (Shinn & Hubbard, 1992) 

 

For public policy towards schools, the case to be made here is firstly that significant learning gains lie within our grasp. The research reported here shows conclusively that formative assessment does improve learning. 

 

The gains in achievement appear to be quite considerable, and as noted earlier, amongst the largest ever reported for educational interventions. 

 

If this first point is accepted, then the second move is for teachers in schools to be provoked and supported in trying to establish new practices in formative assessment, there being extensive evidence to show that the present levels of practice in this aspect of teaching are low (Black, 1993b; McCallum et al., 1993), and that the level of resources devoted to its support, at least in the UK since 1988, has been almost negligible (Daugherty, 1995). 

 

Significant gains can be achieved by many 

 

different routes, and initiatives here are not likely to fail through neglect of delicate and subtle features. 

 

This last point is very important because there does not emerge, from this present review, any one optimum model on which such a policy might be based. What does emerge is a set of guiding principles, with the general caveat that the changes in classroom practice that are needed are central rather than marginal, and have to be incorporated by each teacher into his or her practice in his or her own way (Broadfoot et al., 1996). That is to say, reform in this dimension will inevitably take a long time, and need continuing support from both practitioners and researchers. 

 

BLACK, P.J. (1993b) Formative and summative assessment by teachers, Studies in Science Education, 21, pp. 49-97. 

 

BUTLER, R. (1988) Enhancing and undermining intrinsic motivation; the effects of task-involving and ego-involving evaluation on interest and performance, British Journal of Educational Psychology, 58, pp. 1-14. 

 

CROOKS, T.J. (1988) The impact of classroom evaluation practices on students, Review of Educational Research, 58, pp. 438-481. 

 

FONTANA, D. & FERNANDES, M. (1994) Improvements in mathematics performance as a consequence of self-assessment in Portuguese primary school pupils, British Journal of Educational Psychology, 64, pp. 407-4 

 

FREDERIKSEN, J.R. & WHITE, B.J. (1997) Reflective assessment of students' research within an inquiry-based middle school science curriculum, paper presented at the Annual Meeting of the AERA Chicago 1997 

 

FUCHS, L.S. & FUCHS, D. (1986) Effects of systematic formative evaluation: a meta-analysis, Exceptional Children, 53, pp. 199-20 

 

JOHNSTON, P., GUICE, S., BAKER, K., MALONE, J. & MICHELSON,N. (1995) Assessment of teaching and learning in literature-based classrooms, Teaching and Teacher Education, 11, pp. 359-371. 

 

MARTINEZ, J.G.R. & MARTINEZ, N.C. (1992) Re-examining repeated testing and teacher effects in a remedial mathematics course, British Journal of Educational Psychology, 62, pp. 356-3 

 

MESSICK, S. (1989) Validity, in: R.L. LINN (Ed.) Educational Measurement, 3rd edn, pp. 12-103 (London, Collier Macmillan). 

 

RADNOR, H.A. (1994) The problems of facilitating qualitative formative assessment in pupils, British Journal of Educational Psychology, 64, pp. 145-16 

 

SCHUNK,D.H. (1996) Goal and self-evaluative influences during children's cognitive skill learning, American Educational Research Journal, 33, pp. 359-38 

 

SHEPARD, L.A (1995) Using assessment to improve learning, Educational Leadership, 52(5), pp. 38-43. SHEPARD, L.A., FLEXER, R.J., HIEBERT, E.J., MARION, S.F., MAYFIELD, V. & WESTON, TJ. (1994) Effects of introducing classroom performance assessments on student learning, in: Proceedings of Annual Meeting of AERA Conference, New Orleans: Available from ERIC ED 390918. SHEPARD, L.A., FLEXER, R.J., HIEBERT, E.J., MARION, S.F., MAYFIELD, V. & WESTON, T.J. (1996) 

 

Effects of introducing classroom performance assessments on student learning, Educational Measurement Issues and Practice, 15, pp. 7—18. 

 

STEFANI, L.A.J. (1994) Peer, self and tutor assessment: relative reliabilities, Studies in Higher Education, 19, pp. 69-75. 

 

TORRIE, I. (1989) Developing achievement based assessment using grade related criteria; Research in Science Education, 19, pp. 286-29 

 

WHITING, B., VAN BURGH, J.W. & RENDER, G.F. (1995) Mastery learning in the classroom, paper presented at the Annual Meeting of the AERA San Francisco 1995, available from ERIC ED382688. 

 




%% Import Date: 2023-07-31T05:44:31.927-07:00 %%

---
DOI: 10.1080/02602938.2022.2069674
Date: 2022
Rating: 0/5
Title: "Designing assessment in a digital world: an organising framework"
ShortSummary: ""
annotation-target: bearmanDesigningAssessmentDigital2022.pdf
tags:
  - "#the-digital"
---

#complexity 
#### [Designing assessment in a digital world: an organising framework](bearmanDesigningAssessmentDigital2022.pdf)
**



> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Bearman, M., Nieminen, J., & Ajjawi, R. (2022). Designing assessment in a digital world: An organising framework. _Assessment & Evaluation in Higher Education_, _Ahead of Print_. [https://doi.org/10.1080/02602938.2022.2069674](https://doi.org/10.1080/02602938.2022.2069674)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 


#### Related

#### Annotations


Digital technologies have transformed society, but assessment design may not have kept pace with the digital world. Educational technologies tend to reproduce established academic practices (Selwyn 2014) and, while there may be new means of implementation, such as learning management systems or online proctoring, the fundamentals of assessment design appear to remain constant (Bearman, Boud, et al. 2020). 

 

University teachers consider technology-supported assessment to be “contemporary and innovative” (Bennett et al. 2017, 676) and this view is likely be a key driver for many technology-based assessment designs. However, technology in and of itself is not a panacea, it needs to take account of the complex social realities of educational practice (Kearsley 1998; Selwyn 2010). 

 

According to Bayne (2015, 17), educational technologies are “generally described in instrumental or essentialist terms which either subordinate social practice to technology or subordinate technology to social practice”. At the same time, assessment designers and assessment researchers cannot ignore digital technologies. Technological innovations do allow assessment designs to function in ways that they could not do before. Some kinds of assessments are no longer relevant to contemporary workplaces which are immersed in digital technologies. This paper therefore offers an organising framework to aid navigating the complex intersections between a digital world and assessment design. 

 

A challenge facing researchers and practitioners is the debate surrounding the term “technology” within educational settings (Bayne 2015): a choice of words can imply particular usages that restrict shared understandings (e.g. computer-assisted, computer-based, etc). This paper therefore offers an alternative discourse: we use the term ‘the digital’. This reflects the duality of the digital being both a technology and a social practice. The digital therefore also encompasses practices that are necessary for living with technology, such as for example teamwork and collaboration, that do not in and of themselves focus on using technology. The phrase designing the digital into assessment reflects our primary intention: to support the development of assessment designs relevant to a digital world. 

 

For many years, the intersection between the digital and assessment has commonly been taken to mean e-assessment: “all the assessment tasks using a computer or the Web” (Guàrdia, Crisp, and Alsina 2017, 38). The literature provides many implicit and explicit ways of categorising this work. The most common frameworks appear to rely on assessment task format (e.g. quizzes, virtual simulations and peer assessment software (see Guàrdia, Crisp, and Alsina (2017) or Jisc resources (jisc.ac.uk)). Another organising principle is by approach to testing; for example, a simple quiz or a highly sophisticated ‘intelligent’ system’ (Guàrdia, Crisp, and Alsina 2017). Overall, the emphasis of these frameworks is on practical improvement or more efficient performance. 

 

Therefore, e-assessment is often categorised by purpose such as plagiarism detection or reporting results (Pachler et al. 2009). Moreover, according to the broadest definition, e-assessment now includes almost all assessment designs. For example, almost all universities record grades electronically and learning management systems (LMSs) commonly present assessment tasks. 

 

We also take a holistic view of assessment design. Assessment design concerns the materials associated with describing singular assessment tasks, associated feedback design, grading processes, and how the various assessment elements work in relation to each other. Here, we go beyond a task genre (e.g. quiz, essay) to the specific assessment designed for a unique time and place. Educators design assessments influenced by their circumstances, such as their disciplines and their relationship with colleagues and infrastructure (Bearman et al. 2017). These circumstances also affect how educators work with technology in their assessment designs, responding to their perceptions of the costs, student responses, their desire to be innovative and how they managed logistical challenges (Bennett et al. 2017). 

 

These studies reinforce the notion that assessment and technology are not a-contextual but are shaped by the circumstances of their use. 

 

In Bennett et al.’s (2017) study, university educators describe two main reasons for employing digital technology as part of assessment design. Firstly, they see it providing efficiencies (real or imagined). Secondly, they describe its innovative nature and the need to be “contemporary”. However, beyond this work, there is a striking absence in the literature. Moreover, there is little information to guide educators who wish to take account of the digital in assessment design. While there are helpful well-known educational technology and assessment frameworks, the former do not appear to focus on assessment and the latter do not appear to consider the digital. 

 

While we are mostly concerned with deliberately designing the digital into assessment, we live in a digitally-mediated society. Therefore, almost all university assessment and grading relies on software and hardware platforms. Drawing from Bogost and Montfort (2007, 176), we consider digital platforms to be the software and hardware “that supports other programs”. In this way, 

 

digital platforms are those which assessment designers cannot alter; educators must employ this software and hardware irrespective of assessment design. Examples include using software to detect plagiarism, learning analytics to predict at-risk students, and even uploading student grades into student record systems. While these digital platforms can be changed, generally speaking they are fixed constraints that educational designs need to take into consideration (Bearman, Lambert, and O’Donnell 2021). We therefore do not include these software platforms within our proposed framework but treat them as part of the general context for assessment design, alongside policies and departmental norms (Bearman et al. 2017). 

 

Assessment design in a digital world: an organising framework 

 

We now detail an organising framework for designing the digital into assessment. It is divided into three purposes as outlined within Figure 1. The first purpose focusses on employing the digital – as a tool to support better assessment. The second purpose considers how assessment should concern engaging with digital technologies as a key part of modern society, that is to develop students’ digital literacies. The third purpose looks to how students increasingly need to develop uniquely human capabilities to complement the work that digital technologies will be performing into the future. 

 

These purposes can be held simultaneously and elements can be in tension. Under each purpose, we outline considerations that can assist educators and researchers in conceptualising why and how they will design assessments in a digital world. 

 

Purpose 1: Digital tools for better assessment 

 

This first and most obvious purpose for employing digital technologies is as a tool to improve assessment or assessment processes, including feedback. This purpose appears both self-evident and predominant. For example, educators design digitally-mediated assessment tasks, such as e-portfolios, wikis, video tasks and so on, to improve student learning through assessment. 

 

Therefore, within this broad purpose, we distinguish three areas of consideration for assessment designers: assessment rationales, level of digital enhancement and potential harms. Table 1 provides key questions associated with these considerations. 

 

The first is to credential a student through demonstrated attainment, often called assessment of learning. The second rationale is to promote student development through completing the assessment task, often called assessment for learning. The third rationale is to ensure the assessment leads to longer term development for the student. This is generally called sustainable assessment and is associated with building the students’ evaluative judgement. 

 

Level of digital enhancement 

 

The next consideration revolves around how the technology is employed to fulfil the educators’ objectives. Drawing from the general educational technology literature, we describe the substitution, augmentation, modification and redefinition (SAMR) framework, which offers a useful way of articulating the purposeful integration of the technological tool into task design. SAMR divides the uses of technology into a four-level hierarchy (Puentedura 2009) of increasing pedagogic enhancement. 

[[SAMR seems to be fundamentally an example of positivity bias.]] 

At the first level of SAMR, educators use the technology as substitution for a non-technological task, although employing a new digital technology now usually means replacing another. For example, consider a multiple-choice examination which exchanges a 

 

machine-readable book to one where questions are asked via a tablet. This new design simply substitutes an older technology (pen, paper and optical scanner) for an alternative (digital presentation). There is no fundamental change in what students are asked to do, although frequently, as in our example, it may be more efficient. 

 

The next level is augmentation, where the task mostly remains the same, but it is enhanced by the technology. Drawing from the example above, this might be when the tablet randomly selects different multiple-choice questions or provides students with automated pre-populated feedback following the examination. 

 

The third level is modification, which presents a substantially different task: for example, the test items adapt to the students’ choices, hence allowing the examination to be based around authentic and challenging scenarios. 

 

The fourth level, redefinition, means that a digital task provides a completely different way of working. In our example, the examination format could be replaced by an online simulation where teams of students “role-play” stakeholders addressing a complex real-world scenario, such as damming a major river system (Hirsch and Lloyd 2005). 

 

Potential harms 

 

As mentioned, technology is usually associated with a belief that innovation will make a positive difference or, at least, that it will be neutral. For example, there is no SAMR category describing how the digital might make the student experience worse not better. 

 

This is a limitation associated with this purpose: if you are intending to develop an improvement, it becomes easy to forget that technologies can have unintended and unproductive impacts, such as steering instrumental student behaviours (Henderson, Selwyn, and Aston 2017). For example, an online proctoring program may enhance assessment security but simultaneously negatively impact student experience through raised anxiety levels (Woldeab and Brothen 2019). Similarly, inaccessible assessment designs are a serious concern – no matter how innovative or transformative – as they might further exclude students with disabilities. 

 

However, as assessment designs tend to be iterative (Bearman et al. 2017), evaluating for initial unintended negative impacts (and incorporating findings into future designs) becomes very important. This requires a shift from a romanticised view of technology-mediated assessments (Bennett et al. 2017) to a more nuanced perspective. The corollary to this is that such articulation of harms should be more commonly reported in scholarly forums and publications. 

 

Purpose 2: Developing and credentialling digital literacies 

 

The second purpose for designing the digital into assessment is to promote and credential how students engage with technologies. In this instance, the assessment design is aligned with learning outcome(s) that specifically concern how students engage with digital technologies. 

 

Central to this purpose is that higher education can intentionally 

 

teach technologically-focussed ways of knowing and doing and that this is a matter for assessment design. In other words, digital literacies also require assessment and feedback, the same as any other form of teaching. 

 

Assessment and feedback do not feature heavily in the discussion of digital literacies in higher education (Littlejohn, Beetham, and McGill 2012) and, similarly, research on assessment literacies rarely explicitly address the role of digital technologies (e.g. Smith et al. 2013). 

 

However, there is an extensive body of work on digital literacies in general higher education, including two recent systematic reviews. These describe how digital literacies range from specific skills in using particular software programs (Spante et al. 2018) to developing complex modes of meaning making (Lea and Jones 2011) to surfacing the impacts of datafication surveillance (Raffaghelli and Stewart 2020). Spante et al.’s (2018) review suggests that conceptions of literacies are broad-ranging and indeed ambiguous, while Raffaghelli and Stewart (2020) suggest that particular terms such as ‘data literacy’ often lend themselves to instrumental views. 

 

Mastery or proficiency 

 

Those designing digital in assessment may wish to consider how their designs build mastery or proficiency with respect to digital literacies. 

 

On a less instrumental note, O’Donnell (2020) proposes that digital assessment designs should ideally prompt knowledge creation rather than replication or consumption; for example, mastering knowledge production with blogs, wikis and social media. 

 

Educators should not assume that by undertaking a particular task, students will osmotically master digital skills. Thus, digital literacies must be incorporated in some form into teaching. It is unfair to expect outcomes based purely on what learners bring to a course of study; assumed digital literacies can form barriers for equity groups (Wickens and Miller 2020). 

 

Evaluation and critique 

 

Another consideration for assessment design is how to promote students’ capability to evaluate and critique the digital. This presents an opportunity. 

 

While critical approaches to digital 

 

literacies are very rare within the assessment literature they are more frequently seen within the data literacies literature, prompted by concerns about the rising integration of big data and artificial intelligence techniques into society (e.g. Pangrazio and Selwyn 2019; Williamson, Bayne, and Shay 2020). 

 

An assessment design example is within a Masters of Digital Education, where students interact with presentations of their own digital data, critically reflecting on how educational data is synthesised and communicated (Knox 2017). In this instance, the critical reflections about data form a key part of assessment design. 

 

Purpose 3: Developing and credentialling human capabilities for a digital world 

 

The last purpose focusses on the broader capabilities, beyond digital literacies, required for living and working in a digital world. These capabilities can be distinguished by their emphasis on uniquely human characteristics: human contributions to society that cannot be performed by autonomous machines. 

 

We also suggest assessment designers should consider ‘future-authentic assessment’ that represents the “likely future realities” (Dawson and Bearman 2020, 292). 

 

This is the least described purpose in the assessment literature, with almost no examples of actual task designs. We therefore elaborate more fully. We first briefly outline the well-known twenty first century skills literature and then distinguish what we mean by uniquely human capabilities for a digital world. 

 

We wish to distinguish the uniquely human capabilities explicitly linked to a digital world from these generic twenty first century skills notions of creativity or critical thinking. We define uniquely human capabilities for a digital world as those capabilities which can be conceptualised differently from pre-digital times. By human capability, we mean "an integration of knowledge, skills, personal qualities and understanding used appropriately and effectively-not just in familiar and highly focused specialist contexts but in response to new and changing circumstances" (Stephenson 1998, 2). 

 

We offer two considerations for assessment designers, with respect to developing capabilities for 1) future activities and 2) future self. As with digital literacies, we note that teaching, 

 

feedback and assessment must be aligned here: an assessment task in and of itself will not build these types of capabilities without being deeply embedded within a curriculum (and experienced by students as such). 

 

Future activities The first consideration for assessment designers is to reflect on how a digital world impacts upon our students’ future activities beyond developing digital literacies. For example, Bearman and Luckin (2020) describe how assessment might respond to the rise of artificial intelligence (AI) as a fundamental part of our society. In particular, they note that AI has some significant deficiencies: machines cannot define quality or set standards. Therefore, assessment designs that focus on promoting the uniquely human capability of defining quality or setting criteria may become more significant for a digital world (Bearman and Luckin 2020). 

 

Future self A sense of self is a human characteristic, not possessed by machines in the foreseeable future. Therefore, we suggest that educators should consider how assessment can orient students towards a future self in a digital world. Such assessment designs emphasise what students are becoming rather than what they are doing or will do in the future. For example, an assessment can ask students to dynamically and strategically construct a professional self within relevant online social networks and communities (Ajjawi, Boud, and Marshall 2020). 

 

By offering key considerations around three purposes, assessment design can both employ technology as a tool and integrate sociotechnical dimensions. We note three advantages. Firstly, the framework integrates key concepts from two different literatures: digital technologies and assessment design. Secondly, the framework provides a means whereby educators can make their assessments digitally authentic – that is reflective of the technologies and skills required for an (already) digital world. Finally, this approach shifts developing students’ 

 

literacies and capabilities from an implicit part of assessment design to highlighting these as necessary learning outcomes within a digital world. 

 

In this example, we review an implementation of an e-portfolio: a form of assessment that enables students to demonstrate their learning in diverse and multimodal ways (e.g. text, video, images). Our case is set in a masters level course about digital assessment in mathematics education (approximately 20 students), taught by JHN (second author) and Dr Matti Pauna at the University of Helsinki in 2018. The e-portfolio was intended to be an example of authentic assessment. 

 




%% Import Date: 2023-04-15T15:07:29.868-07:00 %%

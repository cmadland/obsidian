---
DOI: 10/gjsn3g
Date: 2016
Rating: 0/5
Title: "Support for assessment practice: developing the Assessment Design Decisions Framework"
ShortSummary: ""
annotation-target: bearmanSupportAssessmentPractice2016.pdf
---


#### [Support for assessment practice: developing the Assessment Design Decisions Framework](bearmanSupportAssessmentPractice2016.pdf)
**



> [!tldr] Summary
> A short summary - or an abstract in 3 sentences, relating to YOU. What did YOU find interesting about this paper. 

> [!cite] Bibliography
>Bearman, M., Dawson, P., Boud, D., Bennett, S., Hall, M., & Molloy, E. (2016). Support for assessment practice: Developing the Assessment Design Decisions Framework. _Teaching in Higher Education_, _21_(5), 545–556. [https://doi.org/10/gjsn3g](https://doi.org/10/gjsn3g)

> [!quote] Quotable
> Imagine you would quote this paper in your publication. How would you do it? It is probably just one sentence followed by the reference. It is the most intense condensation of the information in this paper and forces you to be on point. 
> 
> You can have multiple alternatives. 


#### Aim of Paper


#### Key insights 


#### Related

#### Annotations


Assessment and feedback can be troublesome areas for university educators, but this is not through want of higher education scholarship. A range of conceptual and empirical publications informing assessment are readily available to most university teachers (e.g. Gore et al. 2009; Boud 2010; Van der Vleuten et al. 2012; Falchikov 2013). In general, the literature focuses more on the learner and their experience of assessment (e.g. Bailey and Garner 2010; Nicol 2010; Shipman et al. 2012) and less on the central role of the educator in designing, implementing and judging assessments. This creates a conundrum for those who wish to improve assessment: how to keep the focus on the learner while including the educator who holds the primary responsibility for assessment. This paper describes the conceptualisation and development of a learner-focused resource, which supports educators’ agency in making considered, nuanced and effective assessment design choices. 

 

Developing assessment literacy seems a reasonable aim, but on further inspection may only be one part of the solution. Studies into conceptions of assessment indicate the significant variation in how individual academics think about assessment (Fletcher et al. 2012) and how these conceptualisations can be at odds with what academics do (Norton, Norton, and Shannon 2013). Offerdahl and Tomanek (2011) describe how individuals’ changed thinking about assessment may not lead to changed teaching practices. In this case study, three educators considered studentcentred strategies, such as using formative assessment that revealed information about students’ progress to inform teaching. These strategies were implemented, but ultimately realigned to the previous didactic approach that focused on students’ provision of correct ‘answers’. Offerdahl and Tomanek (2011) speculate that a stronger degree of dissatisfaction with the status quo may be required to stimulate genuine change. Their study highlights three issues. First, it is more difficult to change assessment practices than it is to change theoretical understandings. Second, there are little data that reveal the reasons for educators’ assessment choices. Finally, if institutions and departments wish to support individual educators to improve assessments, they must consider the influence of the many contextual factors which shape educator practice. 

 

Then, as at present, we define university assessment as the graded and non-graded tasks, undertaken by an enrolled student as part of their formal study, where the learner’s performance is judged by others (teachers or peers). 

 

Like Price et al. (2011), we hold the premise that, while assessment strategies should balance complex and interdependent purposes including accreditation and portrayal of achievements, assessment activities should focus on learning and discourage mechanical approaches to study. 

 

We also claim that assessment practices should develop learners’ own capacities to evaluate their own work to prepare them for future challenges beyond the support of teachers and courses. 

 

Assessment necessarily directs learners’ efforts to mastering the ‘rules of the game’, whether that be writing an essay, answering a multiple-choice question, or conducting an interview. It does so in ways that are not neutral; assessment always acts as an intervention into student learning. 

 

We hold that feedback processes are critical to effective learning through assessment (Nicol and Macfarlane-Dick 2006) and that iterative opportunities for learners to incorporate feedback is a key component of effective practice (Boud and Molloy 2013). These views, while contemporary, are uncontroversial and well represented in the literature. 

 

There are few publications examining the processes educators undertake to optimally design and judge assessments in complex practice environments. As discussed earlier, studies indicate a gap between what educators conceptualise as good assessment practice and what they actually do (Offerdahl and Tomanek 2011; Norton, Norton, and Shannon 2013), however these studies offer only limited insight into why the educators ultimately failed to change their practices. In his study of general teaching practice,Eley (2006) examined university educators’ thinking when planning for teaching. He concluded that specific decisions stem from contextualised teaching repertoires, rather than abstract principles of ‘good teaching’. 

 

We noted that different decisions about assessment occur at different points in the lifespan of a program, and different people make these decisions for different purposes. Some assessment decisions are made at a policy level (e.g. maximum weightings mandated for exams), often by senior staff, who may have no direct relationship with students and are independent of an actual course. 

 

Other assessment decisions are made during the design of the unit or module (e.g. types of task and criteria for success), usually by univer- sity teachers as individuals or in teams, who have some relationship or responsibility for the unit or overall course. We clustered these decisions into a ‘design phase’. 

 

Finally, there is assessment in the form of day-to-day judgements of student work (e.g. types of feedback and grades given to a particular student), often made by assessors who may be tutors or colleagues without responsibility for the assessment design. 

 

All three types of assessments decisions – policy, design and judgement – are significant but, given that they are taken by different people at different times, require different sup- ports. The focus of this project was specifically on assessment design decisions, but under- stood within this broader context. ‘Assessment design decisions’ can then be defined as the corpus of choices regarding assessment, made by university educators who take responsibility for the module or unit or overall program at a curricular level. 

 

The initial 10 interviews exposed the thoughtful commitment of the educators to develop meaningful and valuable assessments. There was a wider range of tasks than we anticipated. For example, participants described role-plays (geography), site visits (education), interviews (journalism) and reflections on videotaped practice (physiotherapy), as well as more traditional forms such as exams (biological sciences) and essays (social work). 

 

Particularly, the data indicated that the foundation of an assessment task was most frequently drawn from a previous task. This included assessments experienced by the educator as a student or implemented at another institution. Most commonly, assessment activities were revised versions of the unit’s previous assessment, sometimes with the expectation of further ‘tweaking’ in the next iteration 

 

In these further 21 interviews, again across four institutions and a range of disciplines, our rationale was to capture more of the routine decision-making involved in assessment design. This second set of data highlighted the distributed nature of assessment design. The person responsible for designing the assessment prior to semester was rarely the person who developed the original paperwork for the unit to be approved. Design was conducted by many individuals, usually with the unit coordinator having primary responsibility, sometimes simultaneously in teams, sometimes sequentially over years 

 

The influence of the overarching course or program was notable; it was harder to change assessment in core units when many other units depended upon them. There appeared to be a real difference in the capacity to change assessments in different situations. The educator leading a decades-old foundational unit could make marginal and incremental changes, whiletheeducatorinstigatinganelectiveunitforthefirsttime had more freedom to innovate. 

 

Educators described the impact of the departmental culture on their assessment practice, particularly the influence of the Head of Department. The latter could promote or discourage innovative assessment design, despite having no apparent immediate responsibility for particular units. In general, however, the data, which were from a broad range of institutions and course types, supported Bennett et al.’s (2011) contention that Australian educators have considerable control over assessment design. 

 

As has been noted elsewhere, the influence of the unit’s disciplinary traditions (such as an established custom of essays or exams) on the assessment design was pervasive (Meyer et al. 2010). What was most striking was that the educators themselves were often unaware of this. 

 

This indicated that the data, while illuminating, were also limited. It illustrated what practice was, not what it might be, because the educators themselves did not have the broader context, or sometimes expertise, to see beyond their immediate circle of circumstance. For example, some participants were more concerned with standards or plagiarism at a micro level than focusing on learning with a particular form of assessment. The data were additionally limited due to its scope; care must be taken not to overgeneralise from a set of interviews in an Australian context to other contexts in which an educator’s opportunities for decision-making may be more constrained. 

 

Assessment and feedback can be troublesome areas for university educators, but this is not through want of higher education scholarship. A range of conceptual and empirical publications informing assessment are readily available to most university teachers (e.g. Gore et al. 2009; Boud 2010; Van der Vleuten et al. 2012; Falchikov 2013). In general, the literature focuses more on the learner and their experience of assessment (e.g. Bailey and Garner 2010; Nicol 2010; Shipman et al. 2012) and less on the central role of the educator in designing, implementing and judging assessments. This creates a conundrum for those who wish to improve assessment: how to keep the focus on the learner while including the educator who holds the primary responsibility for assessment. This paper describes the conceptualisation and development of a learner-focused resource, which supports educators’ agency in making considered, nuanced and effective assessment design choices. 

 

Developing assessment literacy seems a reasonable aim, but on further inspection may only be one part of the solution. Studies into conceptions of assessment indicate the significant variation in how individual academics think about assessment (Fletcher et al. 2012) and how these conceptualisations can be at odds with what academics do (Norton, Norton, and Shannon 2013). Offerdahl and Tomanek (2011) describe how individuals’ changed thinking about assessment may not lead to changed teaching practices. In this case study, three educators considered studentcentred strategies, such as using formative assessment that revealed information about students’ progress to inform teaching. These strategies were implemented, but ultimately realigned to the previous didactic approach that focused on students’ provision of correct ‘answers’. Offerdahl and Tomanek (2011) speculate that a stronger degree of dissatisfaction with the status quo may be required to stimulate genuine change. Their study highlights three issues. First, it is more difficult to change assessment practices than it is to change theoretical understandings. Second, there are little data that reveal the reasons for educators’ assessment choices. Finally, if institutions and departments wish to support individual educators to improve assessments, they must consider the influence of the many contextual factors which shape educator practice. 

 

Like Price et al. (2011), we hold the premise that, while assessment strategies should balance complex and interdependent purposes including accreditation and portrayal of achievements, assessment activities should focus on learning and discourage mechanical approaches to study. 

 




%% Import Date: 2023-06-26T07:51:19.820-07:00 %%
